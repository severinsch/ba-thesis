\chapter{Related Work}
\label{chapter:RelatedWork}

We present other work related to this thesis. We first discuss work related to the analysis of string properties in Section \ref{sec:related:stringproperty}, which is where our implementation fits in. Further, we present different approaches utilizing such methods for the specific application of detecting SQL injection vulnerabilities. Additionally, we present other techniques not closely related to the string properties for detecting such vulnerabilities.


\section{String property analysis}\label{sec:related:stringproperty}

Our approach partially follows the approach by Christensen et al. \cite{brics}. The authors construct a \acl{cfg} from a flow graph, but instead of creating it on-demand, starting at the chosen hotspot node like we do, they consider the total flow graph for grammar creation.
As the total amount of potential hotspots can be far greater than the number of actual points of interest, this total grammar is potentially significantly larger than needed. This increased size reduces the performance of the subsequent steps.
Christensen et al. use the same approximation methods for obtaining regular languages from the generated context free grammars, but instead of directly transforming these grammars into automata like we do, they produce a novel formalism, the \ac{mlfa}.
This automaton allows the extraction of the respective automata for different hotspots. Due to the aforementioned on-demand generation of the grammar, we don't need the extraction capability for single hotspots that the \ac{mlfa} provides. In contrast to our implementation, Christensen et al. also don't transform the obtained automata to regular expressions, but rather provide query options using automata.
The authors provide a feature rich implementation\footnote{https://www.brics.dk/JSA/} of their approach and show that it efficiently produces useful results.

Tabuchi et al. \cite{regex_types} describe a type system for a minimal functional calculus, where strings have a regular expression as their type. Using type inference and reconstruction algorithms, they assign such a type to each string variable. Properties about a variable can then be obtained from its type.
They show that their proposed type system can produce good results when applied to their minimal calculus. While we considered adopting this approach for the analysis, there are some problems, especially due to our different requirements and prerequisites. 

To use the presented approach in practice an (efficient) algorithm for type checking and type reconstruction is needed. 
The given paper does not include these, but rather indicates several problems in constructing such algorithms for the given situation without losing some of the desired preciseness.
The authors mention that using standard type reconstruction by constraint solving for the proposed type system even is impossible due to limitations of regular languages.

Additionally this approach is tailored to the mentioned calculus and utilizes specific features like pattern matching, which would make adapting it to our use case more difficult.

The additional layer of abstraction introduced by the \ac{dfg} used in the approach we chose eliminates this problem and makes adaption easier.

Kirkegaard et al. present a XML transformation system, $\text{X}_{\text{ACT}}$, that functions as a part of the JWIG framework developed by Christensen et al.'s research group \cite{xact}. This system statically verifies the validity of XML transformations by tracking sets of XML values during dataflow analyses.
Comparable to the mentioned approach by Tabuchi et al. \cite{regex_types}, the XML transformation language XDuce also uses regular expression types to describe string values \cite{xduce}. It therefore compares to JWIG as the approach by Tabuchi et al. compares to the previously mentioned work by Christensen et al. \cite{brics}. 
Both of these tools use basic analyses of string values for the purpose of validating XML generation. More advanced approaches like the work by Christensen et al. we adopt provide more robust and more generic results for string analysis.

There exist frameworks to model different analyses using set constraints, for example the BANSHEE toolkit by Kodumal and Aiken \cite{banshee}. BANSHEE allows the user to define a specification of the used constraints, from which a constraint resolution engine is built. Using such a generic framework, different analyses based on constraint resolution can be built, for example pointer analysis or \ac{cfl} reachability. However, Christensen et al. argue that some operations their and by extension our approach use cannot be captured by the constraint operators \cite{brics}.

\section{String property analysis used for SQL injection vulnerability detection}

Gould et al. \cite{gould2004static} build on the work of Christensen et al. and use the obtained automata for further analysis of SQL queries. They use a context-free language reachability algorithm to validate the semantic correctness of SQL queries. For this analysis, they also include the grammar of the SQL language for scoping information and the database scheme for type checking.
The authors show that their tool can precisely detect errors in SQL queries. However they currently do not detect SQL injection vulnerabilities, but rather only assess the semantic correctness and type safety of the queries.

Halfond and Orso also leverage the functionality provided by Christensen et al. to detect SQL injection vulnerabilities using the AMNESIA tool \cite{amnesia}. Their approach differs from the other mentioned works, as it is not a fully static analysis, but rather a static analysis used to enhance the capabilities of runtime monitoring. They use the implementation provided by Christensen et al. to build an SQL query model from the obtained automata. The model represents all of the possible SQL queries that can be generated at the analyzed hotspot.
During runtime, the monitor can now check the dynamically generated queries against the existing query model. If a runtime query does not match the model, the monitor can reject the malicious query to prevent an SQL injection. The authors show in another publication, that their implementation of this dynamic approach can successfully prevent SQL injections in real world use cases without imposing considerable execution overhead on the application \cite{amnesia_evaluation}.

As our approach is able to produce \acp{dfa} for the analyzed code as well, both of the previous approaches could theoretically be adapted to our implementation.

Wassermann and Su \cite{sqli_wassermann_su} present an approach comparable to ours, where they also characterize values of string variables using context free grammars. They specifically target SQL injection vulnerabilities by tracking user-modifiable data and using the generated \acp{cfg} to check whether this data can change the syntactic structure of a query. While this approach is successful in detecting those vulnerabilities, our approach is more general and not focused on detecting one specific type of problem, but rather on providing general information for unspecified further use.
% TODO mehr infor dazu

\section{Other approaches for SQL injection vulnerability detection}

Livshits and Lam present an approach for vulnerability detection based on taint propagation \cite{livshits2005}.
Taint propagation involves finding all sinks derivable from sources via some given derivation rules. For the vulnerability detection for example a function that extracts some user input from a request is a source, while an SQL query execution function is a sink. A sink is flagged as potentially vulnerable if it is derivable from a potentially malicious source via some set of derivation rules. The authors use a precise points-to analysis to find paths that allow unsanitized data to flow into e.g. SQL queries.
As the different sinks, sources and derivation rules for a specific vulnerability need to be provided by the user of their tool, they provide a user friendly way using \ac{pql} to specify vulnerability patterns. 
The authors show that their approach successfully detects previously unknown security vulnerabilities in real world applications.

Another comparable approach by Halfond et al. uses positive tainting \cite{wasp}. Their implementation tracks trusted rather than untrusted strings and allows only tainted strings in queries. The authors argue that this approach is especially suited for preventing SQL injection attacks, as due to the positive tainting incompleteness in the analysis leads to false positives, which can be filtered, instead of false negatives. They show that their implementation of this approach successfully and efficiently detects all tested SQL injection vulnerabilities without false positives.

\section{Overview}

In Table \ref{tab:relatedWorkComparison} we provide an overview of the presented approaches and their respective differences. Here JSA is used as an abbreviation for the approach by Christensen et al., DB for database and PoC for proof of concept. Generally the presented work can be categorized into two groups, with generic approaches that aim to provide information and capabilities for different analyses building on them (\cite{brics}\cite{banshee}\cite{regex_types}) on the one hand side. The other publications try to solve specific problems like preventing SQL injection attacks, sometimes using the aforementioned analysis techniques as a foundation (\cite{gould2004static}\cite{amnesia}\cite{wasp}\cite{xduce}\cite{xact}\cite{livshits2005}\cite{sqli_wassermann_su}).

\begin{table}[H]
	\begin{tabular}{cccc}
		\toprule
		\thead{publication} & \thead{purpose} & \thead{approach} & \thead{provides\\implementation} \\
		\midrule
		\makecell{\cite{brics}} & \makecell{provide general information} & \makecell{JSA} & \makecell{\checkmark} \\
		\midrule
		\makecell{\cite{regex_types}} & \makecell{provide general information} & \makecell{regex type system} & \makecell{} \\
		\midrule
		\makecell{this thesis} & \makecell{provide general information} & \makecell{adaption of JSA,\\Nederhof Alg., state elim.} & \makecell{PoC} \\
		\midrule
		\makecell{\cite{xact}} & \makecell{validate\\XML transformation} & \makecell{simple static string analysis} & \makecell{\checkmark}\\
		\midrule
		\makecell{\cite{xduce}} & \makecell{validate\\XML transformation} & \makecell{regex type system} & \makecell{\checkmark}\\
		\midrule
		\makecell{\cite{banshee}} & \makecell{toolkit for building\\different analyses} & \makecell{set constraint resolution} & \makecell{\checkmark} \\
		\midrule
		\makecell{\cite{gould2004static}} & \makecell{statically verify\\SQL queries} & \makecell{JSA \& \ac{cfl} reachability\\with DB information} & \makecell{prototype} \\
		\midrule
		\makecell{\cite{amnesia}, \cite{amnesia_evaluation}} & \makecell{prevent SQL injection\\attacks} & \makecell{JSA \& runtime monitoring} & \makecell{\checkmark} \\
		\midrule
		\makecell{\cite{sqli_wassermann_su}} & \makecell{prevent SQL injection\\attacks} & \makecell{CFG based,\\detect changes to\\structure of SQL query} & \makecell{prototype} \\
		\midrule
		\makecell{\cite{livshits2005}} & \makecell{prevent SQL injection\\attacks} & \makecell{static taint analysis,\\points-to analysis} & \makecell{\checkmark} \\
		\midrule
		\makecell{\cite{wasp}} & \makecell{prevent SQL injection\\attacks} & \makecell{positive taint analysis} & \makecell{\checkmark} \\
		\bottomrule
	\end{tabular}
	\caption{Comparison of the presented related work}
	\label{tab:relatedWorkComparison}
\end{table}
