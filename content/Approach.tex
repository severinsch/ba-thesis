\chapter{Approach and Implementation}
\label{chapter:Approach}

The general approach for our implementation is adapted from the one described by Christensen et al. \cite{brics}. Conceptually, we first extract a \ac{cfg} from the \ac{dfg}. In multiple steps using different methods we then approximate this grammar into a strongly regular grammar. From this grammar we can create a regular expression object to provide to the user for further analysis.

\tikzstyle{grammar} = [
	rectangle, 
	draw, 
	fill=gray!20, 
	text width=5em, 
	text centered, 
	rounded corners, 
	minimum size=2cm,
	thick
	]
\tikzstyle{graph} = [
	circle, 
	draw, 
	fill=orange!20, 
	text width=5em, 
	text centered,
	minimum size=2cm,
	thick
	]
\tikzstyle{result} = [
	rectangle, 
	draw, 
	fill=green!20, 
	text width=5em, 
	text centered, 
	minimum size=2cm,
	thick
	]
\tikzstyle{line} = [draw, -latex', very thick]
\begin{figure}[H]
	\centering
\begin{tikzpicture}[node distance=1.5cm, auto]
	    \node [graph] (DFG) {\ac{dfg}};
	    \node [grammar, right=of DFG] (CFG) {\ac{cfg}};
	    \node [grammar, right=of CFG, xshift=15mm] (REG) {strongly regular grammar};
	    \node [result, right=of REG] (regex) {regular expression};
	    
	    \path [line] (DFG) -> (CFG);
	    \path [line] (CFG) -> node [midway] {approximation} (REG);
	    \path [line] (REG) -> (regex);
\end{tikzpicture}
\caption{The general approach for obtaining regular expressions}
\end{figure}
\label{fig:approach}

\section{Hotspot Collection} 
We implemented a new \lstinline|Pass| that traverses the \ac{cpg} and collects nodes representing string values which might be of interest for further analysis. This hotspot collection includes all strings that are passed as a query to the Java SQL library and all strings in return statements.

\section{Grammar Creation}
To create the grammar for a given \ac{cpg} node, we traverse the \ac{dfg} backwards, starting at the given node. The starting node can be one of the hotspot nodes collected by the aforementioned \lstinline|Pass|, but in general the grammar creation is independent of the hotspot collection.
For each visited node, we add a \lstinline|Nonterminal| and the fitting productions to our grammar. 

Our Grammar contains the following five types of productions:
\begin{itemize}
	\item \lstinline|UnitProduction|:  $X \rightarrow Y$ for references between nodes
	\item \lstinline|ConcatProduction|: $X \rightarrow Y\ Z$ for concatenation of two nodes
	\item \lstinline|TerminalProduction|: $X \rightarrow \texttt{<terminal>}$ for literal string values and other terminal symbols
	\item \lstinline|UnaryOpProduction|: $X \rightarrow op(Y)$ for unary operations on strings
	\item \lstinline|BinaryOpProduction|: $X \rightarrow op(Y, Z)$ for binary operations on strings
\end{itemize}

Here \texttt{<terminal>} represents a terminal symbol containing a regular expression that describes a string value and "$op$" is a placeholder for a string operation that is applied to some arguments.

\begin{lstlisting}[label={lst:grammar_example}, caption={Example code},escapeinside={(*}{*)}, numbers=right, captionpos=b]
	String s(*\textcolor{red}{$^1$}*) = " foo";
	s(*\textcolor{red}{$^2$}*) = s(*\textcolor{red}{$^3$}*) + "bar";
	s(*\textcolor{red}{$^4$}*) = s(*\textcolor{red}{$^5$}*).trim();
\end{lstlisting}

Consider the code example in Listing \ref{lst:grammar_example} for the following explanations of the different productions.

\lstinline|UnitProduction|s mostly represent references between nodes where the underlying string is not changed. In Listing \ref{lst:grammar_example} this would be the case for the reference from \lstinline|s|$^3$ to the variable declaration in line 1. 

\lstinline|ConcatProduction|s are created for \lstinline|BinaryOperator| nodes that represent string concatenation using the \lstinline|+| operator. For the example in Listing \ref{lst:grammar_example} the nonterminal corresponding to the \lstinline|BinaryOperator| node for the \lstinline|+| in line 2 would have a \lstinline|ConcatProduction| with the right hand side nonterminals corresponding to the nodes for \lstinline|s|$^3$ and the string literal respectively.
% TODO implement append?

\lstinline|TerminalProduction|s point to a \lstinline|Terminal| that represents a fixed regular expression.
For example for the \lstinline|Literal| \ac{cpg} node representing the \lstinline|"bar"| string literal, the corresponding nonterminal has a \lstinline|TerminalProduction| where the \lstinline|Terminal| contains a regular expression that matches only the string "abc". \lstinline|TerminalProduction|s also occur at \ac{cpg} nodes without incoming \ac{dfg} edges where the value is not known. Those nodes could represent any string value and therefore the corresponding \lstinline|Terminal| contains the regular lanuage \lstinline|.*|, matching all strings.

\lstinline|UnaryOpProduction|s and \lstinline|BinaryOpProductions| represent function calls or other operators. The \ac{cpg} for \ref{lst:grammar_example} contains a \lstinline|CallExpression| representing the function call of the library function \lstinline|trim|. We then create an \lstinline|Operation| object representing this operation and the \lstinline|UnaryOpProduction| $X \rightarrow trim(Y)$, where $X$ is the nonterminal corresponding to the node representing \lstinline|s|$^4$ and $Y$ to the one representing \lstinline|s|$^5$. The \lstinline|Operation| objects also contain information about possible arguments and implement the character set transformation and regular approximation needed for the approximation of the grammar described in Section \ref{approximation}. This language agnostic representation of string operation allows developers of the \ac{cpg} library to add support for functions and operators in other languages with different semantics compared to the corresponding Java functions, without needing to change the grammar approximation. For example for the Python expression \lstinline[language=Python]|"abc" * 5| the \lstinline|*| operator can be represented using a generic \lstinline|Repeat| \lstinline|Operation|.

\subsubsection{Improvements}

Unlike Christensen et al. \cite{brics}, we do not consider the total \ac{dfg} when extracting the grammar. While they parse the whole graph into a data structure, to later extract automata for specific nodes, we create the grammar starting from a single node and ignore all parts of the graph not connected via \ac{dfg} edges to this node.

Since often the majority of a large program is not relevant for a specific node, this reduces the amount of nodes we need to handle and the size of the resulting grammar, therefore leading to performance improvements.

Additionally, we can traverse the \ac{dfg} conditionally, stopping at nodes representing numbers. If the traversal reaches such a node, it uses a \lstinline|ValueEvaluator| to try, whether the value the node represents is known. In this case, we can add a \lstinline|TerminalProduction| with the \lstinline|Terminal| representing the value literal and otherwise, if the value is not known, the \lstinline|Terminal| contains a regular expression matching all numbers of the present type, e.g. \lstinline{"0|(-?[1-9][0-9]*)"} for integrals.
	

\section{Regular Approximation}\label{approximation}
\subsection{Character Set Approximation}\label{charsetApprox}
To use the Mohri-Nederhof approximation algorithm \cite{mohri_nederhof}, we need to eliminate all cycles in our grammar that contain operation productions.
All nonterminals are assigned a character set, containing all characters that make up the words in the language of the corresponding nonterminal. Each operation defines a character set transformation - a function $T_{op} : 2^\Sigma \rightarrow 2^\Sigma$ - that approximates how the application of the given operation changes the character set. Here $\Sigma$ represents the set of all possible characters.
For example the character set transformation for a \lstinline|replace| operation, where a known char \lstinline|o| is replaced by a known char \lstinline|n| has the following character set transformation, whereas for a \lstinline|replace| operation, where the newly inserted char is not known, $S$ is transformed to $\Sigma$ if the replaced char is contained in $S$.

\begin{align}
	T_{replace[o, n]}(S) = 
	\begin{cases}
		(S \setminus \{o\}) \cup \{n\}, & \text{if } o \in S\\
		S, & \text{if } o \notin S
	\end{cases}
\end{align}


These approximations, together with the terminals where the character set is known, for example a string literal, can be used in a fixed point computation to assign a character set $C(X)$ to each nonterminal $X$.

To break up the cycles containing operation productions, we replace one operation production $X \rightarrow op(Y)$ in each cycle with a production $X \rightarrow r$, where $r$ is the regular expression that matches the language $C(X)^*$.

We find those operation cycles by viewing the grammar as a graph and determining the \acp{scc} of this graph. Now for each nonterminal $N$ in a given component $C$, we check, whether it has an operation production, and if yes, whether one of the nonterminals on its right-hand side is also part of $C$. If this is the case, by definition of \acp{scc}, $N$ is reachable from this nonterminal and therefore the operation production is part of a cycle.

% TODO mention that graph of SCCs is DAG (and why)?
To determine the \acp{scc}, we use Tarjan's algorithm \cite{tarjan}. This algorithm topologically sorts the returned components in reverse order, which is necessary for the fixpoint computation used to find the charsets.
During the computation, for a given nonterminal $N$, its charset is updated using the charsets of its successors. The reverse topological ordering of the components ensures, that the first handled component is the root in the graph formed by the \acp{scc}, while leafs in this graph are handled last. This ensures that the successors of each nonterminal are either in the same component or in a component that has been handled earlier.

To represent character sets easily, we have two different implementations, both conforming to a common \lstinline|CharSet| interface that requires functions like \lstinline|union : CharSet -> CharSet| and \lstinline|intersect : CharSet -> CharSet|.

The first, \lstinline|SetCharSet|, is mostly a simple wrapper around a \lstinline|Set<Char>| containing the characters.
The second, \lstinline|SigmaCharSet|, is used to easily represent sets like $\Sigma \setminus \{a, b, c\}$ by storing a \lstinline|Set<Char>| containing the characters $not$ contained in the set, while all other characters are assumed to be members.

The behavior of the the set operations \lstinline|union| and \lstinline|intersect| can be described using the following set operations:

\noindent
\begin{alignat*}{3}
	& \text{\lstinline|SigmaCharSet union SigmaCharSet| } && \hat{=} (\Sigma \setminus A) \cup (\Sigma \setminus B) & &= \Sigma \setminus (A \cap B) \\
	& \text{\lstinline|SigmaCharSet union SetCharSet| } && \hat{=} (\Sigma \setminus A) \cup S & &= \Sigma \setminus (A \setminus S) \\
	& \text{\lstinline|SetCharSet union SetCharSet| } && \hat{=} & &\phantom{{}={}} S_1 \cup S_2 \\
	& \text{\lstinline|SigmaCharSet intersect SigmaCharSet| } && \hat{=} (\Sigma \setminus A) \cap (\Sigma \setminus B) & &= \Sigma \setminus (A \cup B) \\
	& \text{\lstinline|SigmaCharSet intersect SetCharSet| } && \hat{=} (\Sigma \setminus A) \cap S & &= S \setminus A \\
	& \text{\lstinline|SetCharSet intersect SetCharSet| } && \hat{=} & &\phantom{{}={}} S_1 \cap S_2
\end{alignat*}

This approach reduces the storage needed to represent the commonly occurring type of character sets, where only a few characters are removed from $\Sigma$. It also simplifies the creation of a regular expression from the character set, since the approach of using a character class containing all characters in the set produces very large character classes for sets with cardinality close to $|\Sigma|$. Using our approach, we can represent a \lstinline|SigmaCharSet| using negated character classes. Since most character sets either contain a comparatively small amount of given chars, or all chars except a few this reduces the average length of the resulting regular expressions. 
For example the \lstinline|SetCharSet| that represents the set $\{\text{'a', 'b', 'c'}\}$ gives us the regular expression \lstinline|[abc]*|, while the \lstinline|SigmaCharSet| representing $\Sigma \setminus \{\text{'0', '1', '2'}\}$ corresponds to \lstinline|[^012]*|.

		
\subsection{Mohri-Nederhof Approximation}

\subsubsection{Strongly Regular Grammars}
Mohri and Nederhof \cite{mohri_nederhof} describe an algorithm to transform a \ac{cfg} into a strongly regular grammar that approximates the given \ac{cfg}.

They define strongly regular grammars as follows:

$\mathcal{R}$ is the equivalence relation defined on the set of nonterminals $N$ of the grammar:

\begin{align}
	 A \mathcal{R} B \Leftrightarrow (\exists \alpha, \beta \in V^* : A \xrightarrow{*} \alpha B \beta) \land (\exists \alpha, \beta \in V^* : B \xrightarrow{*} \alpha A \beta) 
\end{align}

Here $V$ is $\Sigma \cup N$, so the set of all symbols, terminal and nonterminal. $\xrightarrow{*}$ is the reflexive and transitive closure of the production relation $\rightarrow$ defined by the set of productions in the grammar. $A \xrightarrow{*} \alpha B \beta$ means, that there exists a sequence of productions starting at the symbol $A$ to produce a set of symbols that contain $B$. Therefore $\mathcal{R}$ groups all nonterminals into disjoint equivalence classes, where each nonterminal in a class can be produced by each other nonterminal in the class. Those nonterminals are called mutually recursive.

A grammar is strongly regular if the production rules in each such equivalence class are either all right-linear or left-linear.

A production rule is right-linear if it is of the form $A \rightarrow w \alpha$, where $w$ is a sequence of terminal symbols and $\alpha$ is empty or a single nonterminal symbol. Left-linear productions are defined accordingly but nonterminal is on the left side of the production result.

For determining if a production rule of a given equivalence class is right- or left-linear all nonterminals that are not part of the class can be considered as terminals.

Therefore, to transform a \ac{cfg} into a strongly regular grammar, we only need to transform the sets of mutually recursive nonterminals where not all productions are either left-linear or right-linear.

\subsubsection{Transformation}

Mohri and Nederhof describe a more general transformation approach for productions with an arbitrary number of nonterminals on the left hand side \cite{mohri_nederhof}. Since all productions we use have either one or two nonterminals or exactly one terminal on the right hand side, we can reduce this more general approach to the following set of rules described by Christensen et al.\cite{brics}.

For each nonterminal $A$ in a given equivalence class $M$ add a new nonterminal $A'$.
% TODO epsilon production wenn hotspot, bzw bei uns grammar start iwie erwähnen

Replace all productions of $A$ with the following new productions, where $B$ and $C$ are nonterminals in $M$, $X$ and $Y$ are any nonterminals in a different equivalence class and $R$ is a newly created nonterminal.

\noindent
\begin{alignat*}{4}
	& A \rightarrow X 	 && \rightsquigarrow \quad A \rightarrow X\ A'\ \  & &\\
	& A \rightarrow B 	 && \rightsquigarrow \quad A \rightarrow B,\ \ 	   & &B' \rightarrow A'\ &&\\
	& A \rightarrow X\ Y && \rightsquigarrow \quad A \rightarrow R\ A',\ \ & &R  \rightarrow X\ Y\ &&\\
	& A \rightarrow X\ B && \rightsquigarrow \quad A \rightarrow X\ B,\ \  & &B' \rightarrow A'\ &&\\
	& A \rightarrow B\ X && \rightsquigarrow \quad A \rightarrow B,\ \ 	   & &B' \rightarrow X\ A'\ &&\\
	& A \rightarrow B\ C && \rightsquigarrow \quad A \rightarrow B,\ \     & &B' \rightarrow C,\ &&C' \rightarrow A'\\
	& A \rightarrow \texttt{terminal} && \rightsquigarrow \quad A \rightarrow R\ A',\ \ & &R \rightarrow \texttt{terminal}\ &&\\
	& A \rightarrow op(X) && \rightsquigarrow  \quad A \rightarrow R\ A',\ \ & &R \rightarrow op(X)\ &&\\
	& A \rightarrow op(X,Y) && \rightsquigarrow  \quad A \rightarrow R\ A',\ \ & &R \rightarrow op(X,Y)\ &&\\
\end{alignat*}

Since all newly created productions are right-linear, after applying this transformation to all components where it is required, all components in the grammar either contain only left- or only right-linear productions. Therefore the resulting grammar is strongly regular.

\subsubsection{Implementation}

We can view a grammar as a directed graph, with the nonterminals as nodes and an edge from a node $A$ to a node $B$ iff there is a production with $A$ on its left-hand side and $B$ contained in its right-hand side, so a production of form $A \rightarrow \alpha B \beta$.

The aforementioned notion of mutual "reachability", by which $\mathcal{R}$ groups the nonterminals, corresponds to \acp{scc} in this graph view of the grammar.

If two nonterminals $A$ and $B$ are mutually reachable in the graph and therefore part of the same \ac{scc}, there is a sequence of productions to produce $B$ from $A$ and vice versa, which, by definition of $\mathcal{R}$, means they are in the same equivalence class of $\mathcal{R}$.

Thus, to approximate a grammar we view it as a directed graph and find its \acp{scc}, determine the components, where not all productions are of the same linearity and apply the transformation mentioned above to those components.


\section{Transformation to Regular Expression}

\subsection{Strongly Regular Grammar to Automaton}

\subsubsection{Algorithm}

Nederhof describes an algorithm to transform a strongly regular grammar into an equivalent \ac{nfa} in \cite{nederhof}.
More specifically, the algorithm creates an $\epsilon$-\ac{nfa}. The generated automaton always accepts the same language as the given grammar.

The full algorithm can be seen in Algorithm \ref{nederhofAlg}. It creates an automaton $NFA = (K,\Sigma, \Delta, s, F)$ with states $K$, alphabet $\Sigma$, transitions $\Delta$, initial state $s$ and accepting states $F$ from a given \ac{srg} $G = (\Sigma, N, P, S)$ with alphabet $\Sigma$, nonterminals $N$, productions $P$ and a start nonterminal $S$.

The \texttt{create\_state} function used in the pseudo code just creates a new state object which can then be added to the automaton.

Note that for the general algorithm an operation production of form $A \rightarrow op(X)$ is treated like an unary production of form $A \rightarrow X$. The operation productions are always handled by one of the loops in lines\ref{alg:left:firstrec}, \ref{alg:right:firstrec} or \ref{alg:nonrecloop}, because for any operation production initially contained in a cycle, the cycle is broken up by the character set approximation described in Section \ref{charsetApprox}.
Therefore an operation production $C \rightarrow op(DX_1)$ with $C$ and $D$ in the same \ac{scc} can no longer occur.
How the effects of the operation productions are resolved is described in the following section. 

The $\textproc{MAKE\_FA}$ procedure takes two states $q_0$ and $q_1$ and a sequence $\alpha$ of symbols - terminals and nonterminals - and creates an automaton equivalent to the grammar starting at $\alpha$ between those two states.

This recursive process is started in line \ref{alg:initialCall} with the start nonterminal $S$, a newly created initial state $s$ and a newly created accepting state $f$.

For single terminals and $\epsilon$ the algorithm adds an according edge between the two nodes $q_0$ and $q_1$ in lines \ref{alg:eps} to \ref{alg:singleTerm}.

When $\alpha$ contains multiple symbols, a new state $q$ is created and the automaton for the first symbol in $\alpha$ is inserted between $q_0$ and $q$ and the one for the rest of $\alpha$ between $q$ and $q_1$. 

If $\alpha$ consists of just a single terminal $A$, that is not part of any set of mutually recursive nonterminals, so from $A$ there is no sequence of productions to reach $A$ again, we just continue the recursion with the right hand sides of $A$'s productions. The created automaton does not need edges or states corresponding to those single non-recursive nonterminals. 


\begin{figure}[h]
	\begin{minipage}[b]{.45\linewidth}
		\begin{align*}
			&A \rightarrow B\\
			&A \rightarrow C\\
			&B \rightarrow b\\
			&C \rightarrow c
		\end{align*}
		\caption{Example grammar with no recursion}
		\label{fig:nonrec:grammar}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{.45\linewidth}
		\begin{tikzpicture}[
			every initial by arrow/.style = {
				thick,-stealth
			}]
			\node (q0) [state, initial,
			initial text = {}] {$q_0$};
			\node (q1) [state, accepting, right = of q0] {$q_1$};
			\path [-stealth, thick]
			(q0) edge [bend right] node[below] {c}   (q1)
			(q0) edge [bend left] node[above] {b}   (q1);
		\end{tikzpicture}
		\caption{Resulting automaton for the grammar in Figure \ref{fig:nonrec:grammar}}
		\label{fig:nonrec:automaton}
	\end{minipage}
\end{figure}


To show why this is the case, consider the grammar in Figure \ref{fig:nonrec:grammar} creating just the two words $"b"$ and $"c"$. Here $A$, $B$ and $C$ are non-recursive nonterminals, so in the initial procedure call with arguments $(q_0, A, q_1)$ there are just the two recursive calls $\textproc{MAKE\_FA}(q_0, B, q_1)$ and $\textproc{MAKE\_FA}(q_0, C, q_1)$ in line \ref{alg:nonrec}. For those calls again the non-recursive case is chosen, such that for the next recursions $\alpha$ equals $b$ or $c$ respectively, which leads the corresponding edges being created in line \ref{alg:singleTerm}. As demonstrated no edges or states are created for any of the 3 nonterminals, only for the two terminals $a$ and $b$ and the resulting automaton in Figure \ref{fig:nonrec:automaton} accepts the correct language.

For the last remaining case, where $\alpha$ consists of a single nonterminal $A$ that is part of some set of mutually recursive nonterminals $N_i$, the algorithm first adds a new state for each nonterminal in $N_i$ to the graph.

Then we differentiate according to the recursion type of $N_i$, which is obtained by the call to $recursive(N_i)$.

Note that sets with neither left nor right recursion can be handled by either case.

Now for all productions where the left hand side is a nonterminal in $N_i$, a recursive call depending on the right hand side of the production is performed. 
To explain the differences between the recursive calls in the different cases consider the grammars in Figures \ref{fig:rec:grammar:left} and \ref{fig:rec:grammar:right}.

Nederhof only defines the case for left recursion in his publication and states that the else part is "the converse of the then part" \cite{nederhof}. This suggests, that besides switching the condition for the second loop from $C \rightarrow DX_1 \ldots X_m$ to  $C \rightarrow X_1 \ldots X_mD$, switching the order of the states passed to the recursive calls suffices for handling the right recursive case.

However, only changing e.g. $\textproc{MAKE\_FA}(q_0, X_1 \ldots X_m, Q_C)$ leads to incorrect results. Applying this version of the algorithm to a fully right recursive grammar returns a graph with correct states and correct edges with the only difference to a correct solution being, that the start and the end state are switched. To get correct results, besides switching the argument order, all occurrences of $q_0$ as an argument to recursive calls need to be replaced with $q_1$ and vice-versa $q_1$ with $q_0$.

The inverting of the states in the recursive calls leads to the edges between states $q_2$ and $q_3$ of the automata in Figures \ref{fig:rec:automaton1} and \ref{fig:rec:automaton2} being inverted, which has no influence on the accepted language. 

Switching $q_0$ and $q_1$ in the recursive calls is what leads to the needed difference in the resulting automata. In the case of the left recursive grammar, any production sequence of $n$ applications of $B \rightarrow Ab$ has to end with replacing the $A$ on the left hand side of the resulting word with the terminal $a$ to finalize the production rule application. This means that each word has to start with $a$, which is realized in the automaton by adding an edge labeled with $a$ from $q_0$ to $q_3$ due to the recursive call in line \ref{alg:left:firstrec}. For the right recursive grammar conversely, each word has to end with an $a$ due to the $b$s being generated on the left hand side of the $A$ in $B \rightarrow bA$. Therefore an edge from $q_3$ to the finale state $q_1$ is being added by the recursive call in line \ref{alg:right:firstrec}.
Accordingly the corresponding $\epsilon$-edges are added in lines \ref{alg:left:eps} and \ref{alg:right:eps}.

\begin{figure}[h]
\subfigure{
	\begin{minipage}[b]{.5\linewidth}
		\begin{align*}
			&A \rightarrow a\\
			&A \rightarrow B\\
			&B \rightarrow Ab
		\end{align*}
		\caption{Example grammar with left recursion}
		\label{fig:rec:grammar:left}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{.5\linewidth}
		\begin{tikzpicture}[
			every initial by arrow/.style = {
			thick,-stealth
			}]
			\node (q0) [state, initial,
			initial text = {}] {$q_0$};
			\node (q3) [state, right = of q0] {$q_3$};
			\node (q1) [state, accepting, right = of q3] {$q_1$};
			\node (q2) [state, above = of q3] {$q_2$};
			\path [-stealth, thick]
			(q0) edge node[above] {$a$}   (q3)
			(q3) edge[bend right] node[right] {$b$}   (q2)
			(q2) edge[bend right] node[left] {$\epsilon$}   (q3)
			(q3) edge node[above] {$\epsilon$}   (q1);
		\end{tikzpicture}
		\caption{Resulting automaton for the grammar in Figure \ref{fig:rec:grammar:left}}
		\label{fig:rec:automaton1}
	\end{minipage}
}
\subfigure{
	\begin{minipage}[b]{.5\linewidth}
		\begin{align*}
			&A \rightarrow a\\
			&A \rightarrow B\\
			&B \rightarrow bA
		\end{align*}
		\caption{Example grammar with right recursion}
		\label{fig:rec:grammar:right}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{.5\linewidth}
		\begin{tikzpicture}[
			every initial by arrow/.style = {
				thick,-stealth
			}]
			\node (q0) [state, initial,
			initial text = {}] {$q_0$};
			\node (q3) [state, right = of q0] {$q_3$};
			\node (q1) [state, accepting, right = of q3] {$q_1$};
			\node (q2) [state, above = of q3] {$q_2$};
			\path [-stealth, thick]
			(q0) edge node[above] {$\epsilon$}   (q3)
			(q2) edge[bend right] node[left] {$b$}   (q3)
			(q3) edge[bend right] node[right] {$\epsilon$}   (q2)
			(q3) edge node[above] {$a$}   (q1);
		\end{tikzpicture}
		\caption{Resulting automaton for the grammar in Figure \ref{fig:rec:grammar:right}}
		\label{fig:rec:automaton2}
	\end{minipage}
}
\end{figure}

\begin{algorithm}[h]
	\caption{Nederhof Algorithm: \ac{srg} $(\Sigma, N, P, S)$ $\rightarrow$ \ac{nfa} $(K,\Sigma, \Delta, s, F)$}
	\label{nederhofAlg}
	\begin{algorithmic}[1]
		\State $\mathbf{let}$ $\Delta = \emptyset, s = \texttt{create\_state()}, f = \texttt{create\_state()}, F = \{f\}, K = \{s, f\}$
		\State \Call{make\_fa}{$s, S, f$} \label{alg:initialCall}
		
		\Procedure{make\_fa}{$q_0, \alpha, q_1$}
		\If{$\alpha = \epsilon$} \label{alg:eps}
			\State  $\mathbf{let}$ $\Delta = \Delta \cup {(q_0, \epsilon, q_1)}$ \Comment{add $\epsilon$ transition from state $q_0$ to state $q_1$}
		\ElsIf{$\alpha = a,\mathbf{\ some\ }a \in \Sigma$}
			\State  $\mathbf{let}$ $\Delta = \Delta \cup {(q_0, \alpha, q_1)}$ \label{alg:singleTerm}
		\ElsIf{$\alpha = X\beta,\mathbf{\ some\ } X \in V, \beta \in V^* \mathbf{\ such\ that \ }|\beta|>0$}
			\State  $\mathbf{let}$ $q = \texttt{create\_state()};$
			\State \hphantom{$\mathbf{let}$} $K = K \cup \{q\}$ \Comment{create some new state $q$ and add it to the automaton}
			\State \Call{make\_fa}{$q_0,X,q$}
			\State \Call{make\_fa}{$q, X,q_1$}
		\Else
			\State  $\mathbf{let}$ $A = \alpha$ \Comment{$\alpha$ must be a single nonterminal}
			\If{$A \in N_i \mathbf{\ some\ } i$}
				\For{$B \in N_i$}
					\State $\mathbf{let}$ $q_B = \texttt{create\_state()};\ K = K \cup \{q_B\}$
				\EndFor
				% TODO define recursive
				\If{$recursive(N_i) = left$}
					\For{$(C \rightarrow X_1 ... X_m) \in P$ $\mathbf{such\ that}$ $C \in N_i \land X_1, ..., X_m \notin N_i$}
						\State \Call{make\_fa}{$q_0,X_1 ... X_m,q_C$} \label{alg:left:firstrec}
					\EndFor
					\For{$(C \rightarrow DX_1 ... X_m) \in P$ $\mathbf{such\ that}$ $C, D \in N_i \land X_1, ..., X_m \notin N_i$} \label{alg:recnt:left}
						\State \Call{make\_fa}{$q_D,X_1 ... X_m,q_C$}
					\EndFor
					\State  $\mathbf{let}$ $\Delta = \Delta \cup {(q_A, \epsilon, q_1)}$ \label{alg:left:eps}
				\Else
					\For{$(C \rightarrow X_1 ... X_m) \in P$  $\mathbf{such\ that}$ $C \in N_i \land X_1, ..., X_m \notin N_i$}
						\State \Call{make\_fa}{$q_C, X_1 ... X_m,q_1$} \label{alg:right:firstrec}
					\EndFor
					\For{$(C \rightarrow X_1 ... X_mD) \in P$  $\mathbf{such\ that}$ $C, D \in N_i \land X_1, ..., X_m \notin N_i$} \label{alg:recnt:right}
						\State \Call{make\_fa}{$q_C,X_1 ... X_m,q_D$}
					\EndFor
					\State  $\mathbf{let}$ $\Delta = \Delta \cup {(q_0, \epsilon, q_A)}$ \label{alg:right:eps}
				\EndIf
			\Else
				\For{$(A \rightarrow \beta)$} \Comment{A is not recursive} \label{alg:nonrecloop}
					\State \Call{make\_fa}{$q_0,\beta,q_1$}  \label{alg:nonrec}
				\EndFor
			\EndIf
		\EndIf
		\EndProcedure
		
	\end{algorithmic}
\end{algorithm}

\subsubsection{Operation Productions}\label{opProduction}

As described above, operation productions of form $C \rightarrow op(X)$ are treated like a normal production $C \rightarrow X$. 
To apply the effect of the different operations onto the created automaton, we taint all nodes and edges if they are created in recursion calls after an operation production.

If a recursive call $\textproc{MAKE\_FA}(q_0, X, q_C)$ in line \ref{alg:left:firstrec} is caused by an operation production $C \rightarrow op_1(X)$, we pass $op_1$ as a taint to the recursive call. All edges and states created further down this recursion path will be tainted with $op_1$. This allows us to apply the automaton-transformation required to apply the effect of an operation $op_1$ to the sub-automaton tainted with $op_1$.

Consider the grammar in Figure \ref{fig:operations:grammar} and the corresponding automaton in Figure \ref{fig:operations:automaton1}. Here the production $A \rightarrow F$ leads to the creation of the left path including state $q_2$, while for the operation production $A \rightarrow replace[f,x](F)$ the subsequent algorithm calls create the colored path. All colored edges and states are tainted with the $replace$ operation. The created \ac{nfa} has two identical paths, since $A \rightarrow replace[f,x](F)$ is treated like a second $A \rightarrow F$ production, just that the resulting edges and adjacent states are tainted.

After completing the \ac{nfa} creation, we can collect all tainted nodes and apply the automaton transformation defined by $replace[f,x]$ to this sub-automaton consisting of the states $q_0$, $q_3$ and $q_1$.

For the $replace[old, new]$ operation this transformation consists of replacing all occurrences of $old$ on tainted edges with $new$, which gives us the automaton in Figure \ref{fig:operations:automaton2} for the given example.



\begin{figure}[h]
	\begin{minipage}[b]{.3\linewidth}
		\begin{align*}
			&A \rightarrow F\\
			&A \rightarrow replace[f, x](F)\\
			&F \rightarrow fF\\
			&F \rightarrow f\\
		\end{align*}
		\caption{Example grammar with operation production}
		\label{fig:operations:grammar}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{.3\linewidth}
		\begin{tikzpicture}[
			every initial by arrow/.style = {
				thick,-stealth
			}]
			\node (q0) [state, initial, initial where=above,
			initial text = {}, draw = red, fill = red!30] {$q_0$};
			\node (q2) [state, below left = of q0] {$q_2$};
			\node (q3) [state, below right = of q0, draw = red, fill = red!30] {$q_3$};
			\node (q1) [state, accepting, below left = of q3, draw = red, fill = red!30] {$q_1$};
			\path [-stealth, thick]
			(q0) edge node[above] {$\epsilon$}   (q2)
			(q0) edge [draw = red] node[above] {$\epsilon$}   (q3)
			(q2) edge [loop right] node[right] {$f$}   (q2)
			(q3) edge [loop left, draw = red] node[left] {$f$}   (q3)
			(q2) edge node[above] {$f$}   (q1)
			(q3) edge [draw = red] node[above] {$f$}   (q1);
		\end{tikzpicture}
		\caption{Resulting automaton for the grammar in Figure \ref{fig:operations:grammar}}
		\label{fig:operations:automaton1}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{.3\linewidth}
		\begin{tikzpicture}[
		every initial by arrow/.style = {
				thick,-stealth
			}]
			\node (q0) [state, initial, initial where=above,
			initial text = {}] {$q_0$};
			\node (q2) [state, below left = of q0] {$q_2$};
			\node (q3) [state, below right = of q0] {$q_3$};
			\node (q1) [state, accepting, below left = of q3] {$q_1$};
			\path [-stealth, thick]
			(q0) edge node[above] {$\epsilon$}   (q2)
			(q0) edge node[above] {$\epsilon$}   (q3)
			(q2) edge [loop right] node[right] {$f$}   (q2)
			(q3) edge [loop left] node[left] {$x$}   (q3)
			(q2) edge node[above] {$f$}   (q1)
			(q3) edge node[above] {$x$}   (q1);
		\end{tikzpicture}
		\caption{Automaton in figure \ref{fig:operations:automaton1} after applying operation transformation}
		\label{fig:operations:automaton2}
	\end{minipage}
\end{figure}


\subsection{Automaton to Regular Expression}

\subsubsection{State elimination}

To transform the automaton we created from a \ac{srg}, we use the state elimination strategy, also known as the Brzozowski-McCluskey procedure \cite{brzozowksi_mccluskey}.

To apply the procedure, an automaton is first transformed into a \ac{gnfa}. A \ac{gnfa} is an \ac{nfa} where the edges are labeled with regular expressions instead of single symbols.
Also a \ac{gnfa} must only have a single start state and a single end state \cite{hanGNFA}.

To achieve this characteristic, one can add a new start state with a single $\epsilon$ transition to the old start state and a new finale state with incoming $\epsilon$ edges from all previously accepting states.

However, due to the automaton construction using the Nederhof algorithm described above, the automata we obtain already fulfill this property without any need for further modification.
We also use regular expressions as edge labels from the start.


We first replace each pair of edges $(q_0, r_1, q1), (q_0, r_2, q1)$ between two states with a single edge $(q_0, r_1|r_2, q_1)$.
After applying this replacement rule exhaustively, there are no two states $q_0$ and $q_1$ with more than one direct edge between them.

To eliminate a state $q$, we "shortcut" the state by replacing all pairs of transitions $(q',r, q), (q, t, q'')$ with a new transition from $q'$ to $q''$.
The new transition is $(q', rt, q'')$ if $q$ has no loop edge to itself and $(q', rs^*t, q'')$ if it has one with label $s$ \cite{esparza}.

Figures \ref{fig:stateelim:rule2} and \ref{fig:stateelim:rule3} contain examples adapted from Esparza \cite{esparza} that visualize those rules.

After repeatedly applying the two rules and eliminating all other states, the resulting automaton contains only the start and the end state. The single edge between those two states then has the resulting regular expression as a label.

\subsubsection{Delgado heuristic}

The order in which states are eliminated affects the size of the resulting regular expression. There exist different heuristics for choosing an elimination order to minimize the expression size.

We chose a heuristic described by Delgado and Morais \cite{delgado}.
For each state a weight is calculated using the following formula, where $In_q$ is the set of incoming edges of $q$, $Out_q$ the set of outgoing edges, $W_e$ the size of the label on any edge $e$. $Out_q$ and $In_q$ both do not contain a potential loop on $q$. $W_{loop}$ is the size of the loop around $q$ if it exists and $0$ otherwise. 


\begin{align*}
	&weight(q) = 
	&\sum_{e \in In_q} (W_{e} \times (|Out_q| - 1)) + \sum_{e \in Out_q} (W_{e} \times (|In_q| - 1)) + W_{loop} \times (|In_q| \times |Out_q| - 1)
\end{align*}

The weight represents the length of the expression added to the result by removing this state.
Therefore, in each algorithm run, the state with the smallest weight is chosen for elimination.

Delgado and Morais show that using this heuristic produces significantly shorter expressions compared to the naive state elimination. Gruber et al. also show it outperforms almost all other heuristics they considered \cite{gruber}.
Improving the algorithm further by implementing a look-ahead additionally to the heuristic also improves the results, but adds more complexity and impairs the algorithms performance \cite{delgado}.

% further work:
% combine with multiple strategies: https://arxiv.org/pdf/1008.1656.pdf
% 

\begin{figure}
	\centering
	\begin{tikzpicture}
		every initial by arrow/.style = {
			thick,-stealth
		}]
		\node (q0) [state] {$q_0$};
		\node (q1) [state, right = of q0] {$q_1$};
		\node (arrow) [right = of q1] {$\Longrightarrow$};
		\node (q0') [state, right = of arrow] {$q_0$};
		\node (q1') [state, right = of q0'] {$q_1$};
		\path [-stealth, thick]
		(q0) edge[bend left] node[above] {$r_1$}   (q1)
		(q0) edge[bend right] node[below] {$r_2$}   (q1)
		(q0') edge node[above] {$r_1 | r_2$}   (q1');
	\end{tikzpicture}
	\caption{Replacement of edge pairs}
	\label{fig:stateelim:rule2}.
\end{figure}

\begin{figure}
	\centering
	\begin{tikzpicture}
		every initial by arrow/.style = {
			thick,-stealth
		}]
		\node at (0,3) (q0) [state] {$q_0$};
		\node at (0,0) (q1) [state] {$q_1$};
		\node at (2,1.5) (q4) [state] {$q_4$};
		\node at (4,3) (q2) [state] {$q_2$};
		\node at (4,0) (q3) [state] {$q_3$};
		
		\node at (5.5, 1.5) (arrow) {$\Longrightarrow$};
		
		\node at (7,3) (q0') [state] {$q_0$};
		\node at (7,0) (q1') [state] {$q_1$};
		\node at (11,3) (q2') [state] {$q_2$};
		\node at (11,0) (q3') [state] {$q_3$};
		
		\path [-stealth, thick]
		(q0) edge node[above, xshift=2] {$r_1$} (q4)
		(q1) edge node[below, xshift=2] {$r_n$} (q4)
		
		(q4) edge[loop above] node {$s$} (q4)
		
			 edge node[above] {$t_1$} (q2)
			 edge node[below] {$r_m$} (q3)
		
		
		(q0') edge node[above] {$r_1s^*t_1$} (q2')
		(q1') edge node[above left,  xshift=7, yshift=-33] {$r_ns^*t_1$} (q2')
		(q0') edge node[below right, xshift=-29, yshift=35] {$r_1s^*t_m$} (q3')
		(q1') edge node[below] {$r_ns^*t_m$} (q3');
	\end{tikzpicture}
	\caption{Elimination of state $q_4$}
	\label{fig:stateelim:rule3}.
\end{figure}
