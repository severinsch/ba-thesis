\chapter{Approach and Implementation}
\label{chapter:Approach}

The general approach for our implementation is adapted from the one described by Christensen et al. \cite{brics}. Conceptually, we first extract a \ac{cfg} from the \ac{dfg}. In multiple steps using different methods we then approximate this grammar into a strongly regular grammar. From this grammar we can create a regular expression object to provide to the user for further analysis.

\tikzstyle{grammar} = [
	rectangle, 
	draw, 
	fill=gray!20, 
	text width=5em, 
	text centered, 
	rounded corners, 
	minimum size=2cm,
	thick
	]
\tikzstyle{graph} = [
	circle, 
	draw, 
	fill=orange!20, 
	text width=5em, 
	text centered,
	minimum size=2cm,
	thick
	]
\tikzstyle{result} = [
	rectangle, 
	draw, 
	fill=green!20, 
	text width=5em, 
	text centered, 
	minimum size=2cm,
	thick
	]
\tikzstyle{line} = [draw, -latex', very thick]
\begin{figure}[H]
	\centering
\begin{tikzpicture}[node distance=1.5cm, auto]
	    \node [graph] (DFG) {\ac{dfg}};
	    \node [grammar, right=of DFG] (CFG) {\ac{cfg}};
	    \node [grammar, right=of CFG, xshift=15mm] (REG) {strongly regular grammar};
	    \node [result, right=of REG] (regex) {regular expression};
	    
	    \path [line] (DFG) -> (CFG);
	    \path [line] (CFG) -> node [midway] {approximation} (REG);
	    \path [line] (REG) -> (regex);
\end{tikzpicture}
\caption{The general approach for obtaining regular expressions}
\end{figure}
\label{fig:approach}

\section{Hotspot Collection} 
We implemented a new \lstinline|Pass| that traverses the \ac{cpg} and collects nodes representing string values which might be of interest for further analysis. This hotspot collection includes all strings that are passed as a query to the Java SQL library and all strings in return statements.

\section{Grammar Creation}
To create the grammar for a given \ac{dfg} node, we traverse the \ac{dfg} backwards, starting at the given node. The starting node can be one of the hotspot nodes collected by the aforementioned \lstinline|Pass|, but in general the grammar creation is independent of the hotspot collection.
For each visited node, we add a \lstinline|Nonterminal| and the fitting productions to our grammar. 

Our Grammar contains the following five types of productions:
\begin{itemize}
	\item \lstinline|UnitProduction|:  $X \rightarrow Y$
	\item \lstinline|ConcatProduction|: $X \rightarrow Y\ Z$
	\item \lstinline|TerminalProduction|: $X \rightarrow \texttt{<terminal>}$
	\item \lstinline|UnaryOpProduction|: $X \rightarrow op(Y)$
	\item \lstinline|BinaryOpProduction|: $X \rightarrow op(Y, Z)$
\end{itemize}

Here \texttt{<terminal>} represents a terminal symbol containing a regular expression that describes a string value and "$op$" is a placeholder for a string operation that is applied to some arguments.

\begin{lstlisting}[label={lst:grammar_example}, caption={Example code},escapeinside={(*}{*)}, numbers=right, captionpos=b]
	String s(*\textcolor{red}{$^1$}*) = " foo";
	s(*\textcolor{red}{$^2$}*) = s(*\textcolor{red}{$^3$}*) + "bar";
	s(*\textcolor{red}{$^4$}*) = s(*\textcolor{red}{$^5$}*).trim();
\end{lstlisting}

Consider the code example in listing \ref{lst:grammar_example} for the following explanations of the different productions.

\lstinline|UnitProduction|s mostly represent references between nodes where the underlying string is not changed. In \ref{lst:grammar_example} this would be the case for the reference from \lstinline|s|$^3$ to the variable declaration in line 1. 

\lstinline|ConcatProduction|s are created for \lstinline|BinaryOperator| nodes that represent string concatenation using the \lstinline|+| operator. For the example in \ref{lst:grammar_example} the nonterminal corresponding to the \lstinline|BinaryOperator| node for the \lstinline|+| in line 2 would have a \lstinline|ConcatProduction| with the right hand side nonterminals corresponding to the nodes for \lstinline|s|$^3$ and the string literal respectively.
% TODO implement append?

\lstinline|TerminalProduction|s point to a \lstinline|Terminal| that represents a fixed regular expression.
For example for the \lstinline|Literal| \ac{cpg} node representing the \lstinline|"bar"| string literal, the corresponding nonterminal has a \lstinline|TerminalProduction| where the \lstinline|Terminal| contains a regular expression that matches only the string "abc". \lstinline|TerminalProduction|s also occur at \ac{cpg} nodes without incoming \ac{dfg} edges where the value is not known. Those nodes could represent any string value and therefore the corresponding \lstinline|Terminal| contains the regular lanuage \lstinline|.*|, matching all strings.

\lstinline|UnaryOpProduction|s and \lstinline|BinaryOpProductions| represent function calls or other operators. The \ac{cpg} for \ref{lst:grammar_example} contains a \lstinline|CallExpression| representing the function call of the library function \lstinline|trim|. We then create an \lstinline|Operation| object representing this operation and the \lstinline|UnaryOpProduction| $X \rightarrow trim(Y)$, where $X$ is the nonterminal corresponding to the node representing \lstinline|s|$^4$ and $Y$ to the one representing \lstinline|s|$^5$. The \lstinline|Operation| objects also contain information about possible arguments and implement the character set transformation and regular approximation needed for the approximation of the grammar described in \ref{approximation}. This language agnostic representation of string operation allows developers of the \ac{cpg} library to add support for functions and operators in other languages with different semantics compared to the corresponding Java functions, without needing to change the grammar approximation. For example for the Python expression \lstinline[language=Python]|"abc" * 5| the \lstinline|*| operator can be represented using a generic \lstinline|Repeat| \lstinline|Operation|.

\subsubsection{Improvements}

Unlike Christensen et al. \cite{brics}, we do not consider the total \ac{dfg} when extracting the grammar. While they parse the whole graph into a data structure, to later extract automata for specific nodes, we create the grammar starting from a single node and ignore all parts of the graph not connected via \ac{dfg} edges to this node.

Since often the majority of a large program is not relevant for a specific node, this reduces the amount of nodes we need to handle and the size of the resulting grammar, therefore leading to performance improvements.

Additionally, we can traverse the \ac{dfg} conditionally, stopping at nodes representing numbers. If the traversal reaches such a node, it uses a \lstinline|ValueEvaluator| to try, whether the value the node represents is known. In this case, we can add a \lstinline|TerminalProduction| with the \lstinline|Terminal| representing the value literal and otherwise, if the value is not known, the \lstinline|Terminal| contains a regular expression matching all numbers of the present type, e.g. \lstinline{"0|(-?[1-9][0-9]*)"} for integrals.
	

\section{Regular Approximation}\label{approximation}
\subsection{Character Set Approximation}
To use the Mohri-Nederhof approximation algorithm, we need to eliminate all cycles in our grammar that contain operation productions.
All nonterminals are assigned a character set, containing all characters that make up the words in the language of the corresponding nonterminal. Each operation defines a character set transformation - a function $T_{op} : 2^\Sigma \rightarrow 2^\Sigma$ - that approximates how the application of the given operation changes the character set. Here $\Sigma$ represents the set of all possible characters.
For example the character set transformation for a \lstinline|replace| operation, where a known char \lstinline|o| is replaced by a known char \lstinline|n| has the following character set transformation, whereas for a \lstinline|replace| operation, where the newly inserted char is not known, $S$ is transformed to $\Sigma$ if the replaced char is contained in $S$.

\begin{align}
	T_{replace[o, n]}(S) = 
	\begin{cases}
		(S \setminus \{o\}) \cup \{n\}, & \text{if } o \in S\\
		S, & \text{if } o \notin S
	\end{cases}
\end{align}


These approximations, together with the terminals where the character set is known, for example a string literal, can be used in a fixed point computation to assign a character set $C(X)$ to each nonterminal $X$.

To break up the cycles containing operation productions, we replace one operation production $X \rightarrow op(Y)$ in each cycle with a production $X \rightarrow r$, where $r$ is the regular expression that matches the language $C(X)^*$.

We find those operation cycles by viewing the grammar as a graph and determining the \acp{scc} of this graph. Now for each nonterminal $N$ in a given component $C$, we check, whether it has an operation production, and if yes, whether one of the nonterminals on its right-hand side is also part of $C$. If this is the case, by definition of \acp{scc}, $N$ is reachable from this nonterminal and therefore the operation production is part of a cycle.

% TODO mention that graph of SCCs is DAG (and why)?
To determine the \acp{scc}, we use Tarjan's algorithm \cite{tarjan}. This algorithm topologically sorts the returned components in reverse order, which is necessary for the fixpoint computation used to find the charsets.
During the computation, for a given nonterminal $N$, its charset is updated using the charsets of its successors. The reverse topological ordering of the components ensures, that the first handled component is the root in the graph formed by the \acp{scc}, while leafs in this graph are handled last. This ensures that the successors of each nonterminal are either in the same component or in a component that has been handled earlier.

To represent character sets easily, we have two different implementations, both conforming to a common \lstinline|CharSet| interface that requires functions like \lstinline|union : CharSet -> CharSet| and \lstinline|intersect : CharSet -> CharSet|.

The first, \lstinline|SetCharSet|, is mostly a simple wrapper around a \lstinline|Set<Char>| containing the characters.
The second, \lstinline|SigmaCharSet|, is used to easily represent sets like $\Sigma \setminus \{a, b, c\}$ by storing a \lstinline|Set<Char>| containing the characters $not$ contained in the set, while all other characters are assumed to be members.

The behavior of the the set operations \lstinline|union| and \lstinline|intersect| can be described using the following set operations:

\noindent
\begin{alignat*}{3}
	& \text{\lstinline|SigmaCharSet union SigmaCharSet| } && \hat{=} (\Sigma \setminus A) \cup (\Sigma \setminus B) & &= \Sigma \setminus (A \cap B) \\
	& \text{\lstinline|SigmaCharSet union SetCharSet| } && \hat{=} (\Sigma \setminus A) \cup S & &= \Sigma \setminus (A \setminus S) \\
	& \text{\lstinline|SetCharSet union SetCharSet| } && \hat{=} & &\phantom{{}={}} S_1 \cup S_2 \\
	& \text{\lstinline|SigmaCharSet intersect SigmaCharSet| } && \hat{=} (\Sigma \setminus A) \cap (\Sigma \setminus B) & &= \Sigma \setminus (A \cup B) \\
	& \text{\lstinline|SigmaCharSet intersect SetCharSet| } && \hat{=} (\Sigma \setminus A) \cap S & &= S \setminus A \\
	& \text{\lstinline|SetCharSet intersect SetCharSet| } && \hat{=} & &\phantom{{}={}} S_1 \cap S_2
\end{alignat*}

This approach reduces the storage needed to represent the commonly occurring type of character sets, where only a few characters are removed from $\Sigma$. It also simplifies the creation of a regular expression from the character set, since the approach of using a character class containing all characters in the set produces very large character classes for sets with cardinality close to $|\Sigma|$. Using our approach, we can represent a \lstinline|SigmaCharSet| using negated character classes. Since most character sets either contain a comparatively small amount of given chars, or all chars except a few this reduces the average length of the resulting regular expressions. 
For example the \lstinline|SetCharSet| that represents the set $\{\text{'a', 'b', 'c'}\}$ gives us the regular expression \lstinline|[abc]*|, while the \lstinline|SigmaCharSet| representing $\Sigma \setminus \{\text{'0', '1', '2'}\}$ corresponds to \lstinline|[^012]*|.

		
\subsection{Mohri-Nederhof Approximation}

\subsubsection{Strongly Regular Grammars}
Mohri and Nederhof describe an algorithm to transform a \ac{cfg} into a strongly regular grammar that approximates the given \ac{cfg}.

They define strongly regular grammars as follows:

$\mathcal{R}$ is the equivalence relation defined on the set of nonterminals $N$ of the grammar:

\begin{align}
	 A \mathcal{R} B \Leftrightarrow (\exists \alpha, \beta \in V^* : A \xrightarrow{*} \alpha B \beta) \land (\exists \alpha, \beta \in V^* : B \xrightarrow{*} \alpha A \beta) 
\end{align}

Here $V$ is $\Sigma \cup N$, so the set of all symbols, terminal and nonterminal. $\xrightarrow{*}$ is the reflexive and transitive closure of the production relation $\rightarrow$ defined by the set of productions in the grammar. $A \xrightarrow{*} \alpha B \beta$ means, that there exists a sequence of productions starting at the symbol $A$ to produce a set of symbols that contain $B$. Therefore $\mathcal{R}$ groups all nonterminals into disjoint equivalence classes, where each nonterminal in a class can be produced by each other nonterminal in the class. Those nonterminals are called mutually recursive.

A grammar is strongly regular if the production rules in each such equivalence class are either all right-linear or left-linear.

A production rule is right-linear if it is of the form $A \rightarrow w \alpha$, where $w$ is a sequence of terminal symbols and $\alpha$ is empty or a single nonterminal symbol. Left-linear productions are defined accordingly but nonterminal is on the left side of the production result.

For determining if a production rule of a given equivalence class is right- or left-linear all nonterminals that are not part of the class can be considered as terminals.

Therefore, to transform a \ac{cfg} into a strongly regular grammar, we only need to transform the sets of mutually recursive nonterminals where not all productions are either left-linear or right-linear.

\subsubsection{Transformation}

Mohri and Nederhof describe a more general transformation approach for productions with an arbitrary number of nonterminals on the left hand side in \cite{mohri_nederhof}. Since all productions we use have either one or two nonterminals or exactly one terminal on the right hand side, we can reduce this more general approach to the following set of rules described by Christensen et al. in \cite{brics}.

For each nonterminal $A$ in a given equivalence class $M$ add a new nonterminal $A'$.
% TODO epsilon production wenn hotspot, bzw bei uns grammar start iwie erwähnen

Replace all productions of $A$ with the following new productions, where $B$ and $C$ are nonterminals in $M$, $X$ and $Y$ are any nonterminals in a different equivalence class and $R$ is a newly created nonterminal.

\noindent
\begin{alignat*}{4}
	& A \rightarrow X 	 && \rightsquigarrow \quad A \rightarrow X\ A'\ \  & &\\
	& A \rightarrow B 	 && \rightsquigarrow \quad A \rightarrow B,\ \ 	   & &B' \rightarrow A'\ &&\\
	& A \rightarrow X\ Y && \rightsquigarrow \quad A \rightarrow R\ A',\ \ & &R  \rightarrow X\ Y\ &&\\
	& A \rightarrow X\ B && \rightsquigarrow \quad A \rightarrow X\ B,\ \  & &B' \rightarrow A'\ &&\\
	& A \rightarrow B\ X && \rightsquigarrow \quad A \rightarrow B,\ \ 	   & &B' \rightarrow X\ A'\ &&\\
	& A \rightarrow B\ C && \rightsquigarrow \quad A \rightarrow B,\ \     & &B' \rightarrow C,\ &&C' \rightarrow A'\\
	& A \rightarrow \texttt{terminal} && \rightsquigarrow \quad A \rightarrow R\ A',\ \ & &R \rightarrow \texttt{terminal}\ &&\\
	& A \rightarrow op(X) && \rightsquigarrow  \quad A \rightarrow R\ A',\ \ & &R \rightarrow op(X)\ &&\\
	& A \rightarrow op(X,Y) && \rightsquigarrow  \quad A \rightarrow R\ A',\ \ & &R \rightarrow op(X,Y)\ &&\\
\end{alignat*}

Since all newly created productions are right-linear, after applying this transformation to all components where it is required, all components in the grammar either contain only left- or only right-linear productions. Therefore the resulting grammar is strongly regular.

\subsubsection{Implementation}

We can view a grammar as a directed graph, with the nonterminals as nodes and an edge from a node $A$ to a node $B$ iff there is a production with $A$ on its left-hand side and $B$ contained in its right-hand side, so a production of form $A \rightarrow \alpha B \beta$.

The aforementioned notion of mutual "reachability", by which $\mathcal{R}$ groups the nonterminals, corresponds to \acp{scc} in this graph view of the grammar.

If two nonterminals $A$ and $B$ are mutually reachable in the graph and therefore part of the same \ac{scc}, there is a sequence of productions to produce $B$ from $A$ and vice versa, which, by definition of $\mathcal{R}$, means they are in the same equivalence class of $\mathcal{R}$.

Thus, to approximate a grammar we view it as a directed graph and find its \acp{scc}, determine the components, where not all productions are of the same linearity and apply the transformation mentioned above to those components.


\section{Transformation to Regular Expression}

\subsection{Strongly Regular Grammar to Automaton}

Nederhof describes an algorithm to transform a strongly regular grammar into an equivalent \ac{nfa} in \cite{nederhof}.
More specifically, the algorithm creates an $\epsilon$-\ac{nfa}, which contains additional $\epsilon$ edges. The generated automaton accepts the same language as the given grammar.

% TODO operation production???


\begin{comment}

%TODO :(
%- Probleme:
\begin{itemize}
	\item Strongly Regular Grammar -> Regex wsh nicht direkt möglich (nichtmal für normal regular wirklich?)
	\item grammar -> $\epsilon$-NFA -> regex: mindestens zweiter step exponentielles wachstum in größe (eig unnötig?)
	\item CFG, die reguläre Sprache akzeptiert -> finite automaton mit gleicher Sprache: bewiesen unsolvable (ist strongly regular nicht eig kontext frei? :()
	\item siehe limitations in nederhof CFG -> automaton paper
\end{itemize}
Optionen:
\begin{itemize}
	\item Ansatz umstellen: von anfang an wie JSA, am ende MLFA -> DFA extracten -> DFA to regex
	\item nederhof paper: strongly regular grammar -> finite automaton
	\begin{itemize}
		\item Frage: NFA oder DFA? DFA -> regex gut machbar, NFA -> regex idk?
		\item warum sollte NFA sein wenn reguläre sprache akzeptiert
	\end{itemize}
\end{itemize}

\end{comment}
