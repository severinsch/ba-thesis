\chapter{Approach and Implementation}
\label{chapter:Approach}

We first provide a general overview of our approach before explaining the different steps.
% TODO satz verbessern

\section{General Approach}

The general approach for our implementation is adapted from the one described by Christensen et al. \cite{brics}. Conceptually, we first create a \acf{cfg} from the \ac{dfg} in a process described in Section \ref{sec:grammarCreation}.
The created \ac{cfg} is then approximated to a \acf{srg} using the Character Set Approximation described in Section \ref{sec:charsetApprox} and the Mohri-Nederhof algorithm described in Section \ref{sec:mohriNederhofApprox}. We then transform this \ac{srg} into an \acf{nfa} using Nederhof's algorithm described in Section \ref{sec:nederhofAlgorithm}. Finally, in Section \ref{sec:nfa2regex} we describe how to create a regular expression from this automaton using the state elimination strategy. Figure \ref{fig:approach} visualizes this process and the different steps from a graph to different types of formal grammars to a regular expression.


\tikzstyle{grammar} = [
	rectangle, 
	draw, 
	fill=gray!20, 
	text width=5em, 
	text centered, 
	rounded corners, 
	minimum size=2cm,
	thick
	]
\tikzstyle{automaton} = [
	circle, 
	draw, 
	fill=blue!20, 
	text width=5em, 
	text centered,
	minimum size=2cm,
	thick
]
\tikzstyle{graph} = [
	ellipse, 
	draw, 
	fill=orange!20, 
	text width=5em, 
	text centered,
	minimum height=1.5cm,
	minimum width=1cm,
	text width=1.5cm,
	align=center,
	thick
	]
\tikzstyle{result} = [
	rectangle, 
	draw, 
	fill=green!20, 
	text width=5em, 
	text centered, 
	minimum size=2cm,
	thick
	]
\tikzstyle{line} = [draw, -latex', very thick]
\begin{figure}[H]
	\centering
\begin{tikzpicture}[node distance=2cm and 2.5cm, auto]
	    \node [graph] (DFG) {\ac{dfg}};
	    \node [grammar, right=of DFG] (CFG) {\ac{cfg}};	    
	    \node [automaton, below= of CFG] (nfa) {\ac{nfa}};
	    \node [result, left=of nfa] (regex) {regular expression};
 	    \node [grammar, right=of nfa, xshift=1.5cm] (REG) {\ac{srg}};
	    
	    \path [line] (DFG) -> node[above, align=center] {grammar\\creation} (CFG);
	    \path [line] (CFG) -| coordinate (midpoint) (REG);
	    \path [line] (CFG) -> node [above, midway] {charset approximation} (midpoint);
 	    \path [line] (midpoint) -> node [left, midway, align=center] {Mohri-Nederhof\\approximation} (REG);
	    \path [line] (REG) -> node[above, align=center] {Nederhof's\\algorithm} (nfa);
	    \path [line] (nfa) -> node[above, align=center] {state\\elimination} (regex);
\end{tikzpicture}
\caption{The general approach for obtaining regular expressions}
\label{fig:approach}
\end{figure}


\section{Grammar Creation}\label{sec:grammarCreation}
To create the grammar for a given \ac{cpg} node, we traverse the \ac{dfg} backwards, starting at the given node. 
% The starting node can be one of the hotspot nodes collected by the aforementioned \lstinline|Pass|, but in general the grammar creation is independent of the hotspot collection.
For each visited node, we add a \lstinline|Nonterminal| and the fitting productions to our grammar. 

The different types of productions we use can be seen in Table \ref{tab:productions}, where \texttt{<terminal>} represents a terminal symbol containing a regular expression that describes a string value and \enquote{$op$} is a placeholder for a string operation that is applied to some arguments.

\begin{table}[H]
	\centering
	\begin{tabular}{ccc}
		\toprule
		\thead{\textbf{Name}} & \thead{\textbf{Production}} & \thead{\textbf{Usage}} \\
		\midrule
		\makecell{\lstinline|UnitProduction|} & \makecell{$X \rightarrow Y$} & \makecell{references between nodes} \\
				\midrule
		\makecell{\lstinline|ConcatProduction|} & \makecell{ $X \rightarrow Y\ Z$} & \makecell{concatenation of two nodes} \\
				\midrule
		\makecell{\lstinline|TerminalProduction|} & \makecell{$X \rightarrow \texttt{<terminal>}$} & \makecell{literal string values and\\other terminal symbols} \\
				\midrule
		\makecell{\lstinline|OperationProduction|} & \makecell{$X \rightarrow op(Y)$} & \makecell{operations on strings} \\
		\bottomrule
	\end{tabular}
	\caption{Production types we use}
	\label{tab:productions}
\end{table}

Consider the code example in Listing \ref{lst:grammar_example} for the following explanations of the different productions.

\begin{lstlisting}[label={lst:grammar_example}, caption={Grammar creation example code},escapeinside={(*}{*)}, numbers=right, captionpos=b]
	String s(*\textcolor{red}{$^1$}*) = " foo";
	s(*\textcolor{red}{$^2$}*) = s(*\textcolor{red}{$^3$}*) + "bar";
	s(*\textcolor{red}{$^4$}*) = s(*\textcolor{red}{$^5$}*).trim();
\end{lstlisting}

\lstinline|UnitProduction|s mostly represent references between nodes where the underlying string is not changed. In Listing \ref{lst:grammar_example} this would be the case for the reference from \lstinline|s|$^3$ to the variable declaration in line 1. 

\lstinline|ConcatProduction|s are created for \lstinline|BinaryOperator| nodes that represent a string concatenation using the \lstinline|+| operator. For the example in Listing \ref{lst:grammar_example} the nonterminal corresponding to the \lstinline|BinaryOperator| node for the \lstinline|+| in line 2 would have a \lstinline|ConcatProduction| with the right hand side nonterminals corresponding to the nodes for \lstinline|s|$^3$ and the string literal respectively.

\lstinline|TerminalProduction|s point to a \lstinline|Terminal| that represents a fixed regular expression.
For example, for the \lstinline|Literal| \ac{cpg} node representing the \lstinline|"bar"| string literal, the corresponding nonterminal has a \lstinline|TerminalProduction| where the \lstinline|Terminal| contains a regular expression that matches only the string \enquote{bar}. \lstinline|TerminalProduction|s also occur at \ac{cpg} nodes without incoming \ac{dfg} edges where the value is not known. Those nodes could represent any string value and therefore the corresponding \lstinline|Terminal| contains the regular expression \lstinline|.*|, matching all strings.

\lstinline|OperationProduction|s represent function calls or other operators. The \ac{cpg} for Listing \ref{lst:grammar_example} contains a \lstinline|CallExpression| representing the function call to the library function \lstinline|trim|. We create a \lstinline|Trim| object representing this operation and the \lstinline|OperationProduction| $X \rightarrow trim(Y)$, where $X$ is the nonterminal corresponding to the node representing \lstinline|s|$^4$ and $Y$ to the one representing \lstinline|s|$^5$. 
All operation objects like \lstinline|Trim| also contain information about possible arguments and implement a character set transformation and an automaton transformation. These transformations describe the effect of the operation on the set of characters making up the words the operation is applied on or automata accepting those words respectively.
Examples and how these transformations are used for the approximation are described in Section \ref{sec:approximation}.
This language agnostic representation of string operations allows developers of the \ac{cpg} library to add support for functions and operators in other languages with different semantics compared to the corresponding Java functions, without needing to change the grammar approximation. For example for the Python expression \lstinline[language=Python]|"abc" * 5| the \lstinline|*| operator can be represented using a generic \lstinline|Repeat| operation object. For all further steps it is not relevant whether this repeat operation is created from the mentioned Python operator \lstinline[language=Python]|s * n| or from the corresponding Java function \lstinline|s.repeat(n)|. 


\subsubsection{Improvements}

Unlike Christensen et al. \cite{brics}, we do not consider the total \ac{dfg} when extracting the grammar. They parse the whole graph into a grammar describing all nodes, while we create the grammar starting from a single node and ignore all parts of the graph not connected via \ac{dfg} edges to this node.

Since often the majority of a large program is not relevant for a specific node, this reduces the amount of nodes we need to handle during analysis. Consequently, this reduces the size of the resulting grammar, therefore leading to performance improvements.

Additionally, we can traverse the \ac{dfg} conditionally, stopping at nodes representing numbers. If the traversal reaches such a node, we use an existing analysis that tries to compute the precise value.
For example, for an integer created by usual arithmetic operations, this analysis can obtain the resulting value.
In this case, we can add a \lstinline|TerminalProduction| with the \lstinline|Terminal| representing the value literal and otherwise, if the value is not known, the \lstinline|Terminal| contains a regular expression matching all numbers of the present type, e.g. \lstinline{"0|(-?[1-9][0-9]*)"} for integrals.
	

\section{Regular Approximation}\label{sec:approximation}

To transform the created grammar into a regular expression, we need to approximate the \ac{cfg} we obtained like described in the previous section. Since basic regular expressions accept regular languages, the result of this approximation has to be a type of grammar that produces only regular languages to allow direct conversion to regular expressions without losing information. Using the two different approximation steps described in the following section, the \ac{cfg} is approximated into a \acl{srg}.

\subsection{Character Set Approximation}\label{sec:charsetApprox}
To use the Mohri-Nederhof approximation algorithm described in Section \ref{sec:mohriNederhofApprox}, we first need to eliminate all cycles in our grammar that contain operation productions \cite{mohri_nederhof}.

First, we view the grammar as a graph in which each symbol of the grammar corresponds to one graph node and for each production there are edges from the nonterminal on the left hand side to all symbols on the right hand side. 
For two nonterminals $A$ and $B$ there is an edge from the node corresponding to $A$ to the one corresponding to $B$ if and only if there exists a production of form $A \rightarrow \alpha B \beta$ with $\alpha$ and $\beta$ sequences of arbitrary symbols.
This graph allows us to group terminals that are reachable from each other by finding the \acp{scc} of the graph.

All nonterminals are assigned a character set, containing all characters that make up the words in the language of the corresponding nonterminal.

\begin{figure}[!h]
	\begin{align*}
		&S \rightarrow replace[c, x](A)\\
		&A \rightarrow BC | CB\\
		&B \rightarrow "ba"\\
		&C \rightarrow "ca"
	\end{align*}
	\caption{Example grammar}
	\label{fig:charset:grammar}
\end{figure}

We assign a character set to each nonterminal $N$ using a fixed-point iteration inside the graph component of $N$ that constructs a character set $C(N)$ for $N$ from the character sets of the nonterminals on the right hand side of $N$'s productions.

For productions with terminals on the right hand side, the character set is just the set of all characters occurring in the terminal. For the terminals that are regular expressions, we create the character set when we construct the regular expression. For example for the regular expression representing integers mentioned above, the character set contains all digits and the minus sign.
For concatenation productions like $A \rightarrow BC$ in the example above, we take the union of the character sets of the two nonterminals on the right hand side.

For the example grammar in Figure \ref{fig:charset:grammar}, $B$ represents the word $ba$ and and $C$ represents $ca$, therefore the corresponding sets of characters are  $\{\text{'a', 'b'}\}$ and  $\{\text{'a', 'c'}\}$ respectively. The words that can be generated from $A$ are combinations of $B$ and $C$ and therefore always contain all characters in the character sets of $B$ and $C$. Thus, the character set for $A$ is $\{\text{'a', 'b'}\} \cup \{\text{'a', 'c'}\} = \{\text{'a', 'b', 'c'}\}$.

Each operation defines a character set transformation - a function $T_{op} : 2^\Sigma \rightarrow 2^\Sigma$ - that approximates how the application of the given operation changes the character set. Here, $\Sigma$ represents the set of all possible characters.
For example the character set transformation for a \lstinline|replace| operation, where a known character \lstinline|o| is replaced by a known character \lstinline|n| has the character set transformation described in Formula \ref{math:replace:known}.

\begin{align}
	\centering
	T_{replace[o, n]}(S) = 
	\begin{cases}
		(S \setminus \{o\}) \cup \{n\}, & \text{if } o \in S\\
		S, & \text{if } o \notin S
	\end{cases}
	\label{math:replace:known}
\end{align}

In comparison, for a \lstinline|replace| operation, where the newly inserted character is not known, the transformation is defined as shown in Formula \ref{math:replace:unknown}. Here, if the replaced character is contained in $S$, the set is transformed to $\Sigma$, since the newly inserted character could be any element of $\Sigma$.

\begin{align}
	\centering
	T_{replace[o, ?]}(S) = 
	\begin{cases}
		\Sigma & \text{if } o \in S\\
		S, & \text{if } o \notin S
	\end{cases}
	\label{math:replace:unknown}
\end{align}

These approximations are used in the fixed-point computation to assign character sets.
As mentioned above, in the example in Figure \ref{fig:charset:grammar} the character set for $A$ is $\{\text{'a', 'b', 'c'}\}$. To obtain the character set of $S$, we apply the transformation defined by the $replace[c,x]$ operation to the character set of $A$, which gives us $(\{\text{'a', 'b', 'c'}\} \setminus \{\text{'c'}\}) \cup \{\text{'x'}\} = \{\text{'a', 'b', 'x'}\}$ as the character set of $S$.

To determine the \acp{scc}, we use Tarjan's algorithm \cite{tarjan}.
The \acp{scc} of a graph form a \ac{dag}, because if the graph of \acp{scc} would contain a cycle, all contained components would be strongly connected and therefore joined into one component. This \ac{dag} implies that there exists a topological ordering of the \acp{scc} \cite{mit_algorithms}.
Tarjan's algorithm topologically sorts the returned components in reverse order as a byproduct, which is necessary for the fixpoint computation to terminate.
During the computation, for a given nonterminal $N$, its charset is updated using the charsets of its successors. The reverse topological ordering of the components ensures that the first handled component is the root in the graph formed by the \acp{scc}, while leafs in this graph are handled last. This ensures that the successors of each nonterminal are either in the same component or in a component that has already been handled earlier.

To break up the cycles containing operation productions, we replace one operation production $X \rightarrow op(Y)$ in each cycle with a production $X \rightarrow r$, where $r$ is the regular expression that matches the language $C(X)^*$. Here $C(X)$ again denotes the character set we assigned to $X$.

To find the cycles in the grammar, we check for each nonterminal $A$ in a given component $M$, whether it has an operation production, and if yes, whether one of the nonterminals on its right-hand side is also part of $M$. If this is the case, by definition of \acp{scc}, $A$ is reachable from this nonterminal and therefore the operation production is part of a cycle.

\subsubsection{Character Set Implementation}

In real world applications, the occurring character sets usually either contain only a few characters, for example the alphanumerics in a string literal, or almost all characters, for example when a single character is removed from an unknown variable represented by $\Sigma$.

We also need to efficiently convert both of these types of sets into short regular expressions to keep the results human-readable. A naive implementation like joining all contained characters using the regular expression choice operator would lead to extremely long expressions for very large sets.

To solve this problem and easily represent these two extremes, we have two different implementations, both conforming to a common \lstinline|CharSet| interface that requires the common set functions \lstinline|union|, \lstinline|intersect|, functionality for adding and removing characters and membership checks.

The first, \lstinline|SetCharSet|, is mostly a simple wrapper around a \lstinline|Set<Char>| containing the characters.
The second, \lstinline|SigmaCharSet|, is used to easily represent sets like $\Sigma \setminus \{a, b, c\}$ by storing a \lstinline|Set<Char>| containing the characters $not$ contained in the set, while all other characters are assumed to be members.

The behavior of the the set operations \lstinline|union| and \lstinline|intersect| can be described using the operations defined in Figure \ref{fig:setops}.

\begin{figure}
\noindent
\begin{alignat*}{3}
	& \text{\lstinline|SigmaCharSet union SigmaCharSet| } && \hat{=}\ (\Sigma \setminus A) \cup (\Sigma \setminus B) & &= \Sigma \setminus (A \cap B) \\
	& \text{\lstinline|SigmaCharSet union SetCharSet| } && \hat{=}\ (\Sigma \setminus A) \cup S & &= \Sigma \setminus (A \setminus S) \\
	& \text{\lstinline|SetCharSet union SetCharSet| } && \hat{=}\ & &\phantom{{}={}} S_1 \cup S_2 \\
	& \text{\lstinline|SigmaCharSet intersect SigmaCharSet| } && \hat{=}\ (\Sigma \setminus A) \cap (\Sigma \setminus B) & &= \Sigma \setminus (A \cup B) \\
	& \text{\lstinline|SigmaCharSet intersect SetCharSet| } && \hat{=}\ (\Sigma \setminus A) \cap S & &= S \setminus A \\
	& \text{\lstinline|SetCharSet intersect SetCharSet| } && \hat{=}\ & &\phantom{{}={}} S_1 \cap S_2
\end{alignat*}
\caption{Definitions of \lstinline|union| and \lstinline|intersect| using standard set operations.}
\label{fig:setops}
\end{figure}

This approach reduces the storage needed to represent the type of character set, where only a few characters are removed from $\Sigma$ compared to always storing all contained characters. It also simplifies the creation of regular expressions from the character set. As mentioned above, always using all contained characters in the regular expression produces large regular expressions for sets with cardinality close to $|\Sigma|$. Using our approach, we can represent the regular expression created from a \lstinline|SigmaCharSet| using negated character classes and those created from a \lstinline|SetCharSet| using normal character classes. This reduces the average length of the resulting expressions, compared to always using the same approach.
For example the \lstinline|SetCharSet| that represents the set $\{\text{'a', 'b', 'c'}\}$ is used to create the regular expression \lstinline|[abc]*|, while the \lstinline|SigmaCharSet| representing $\Sigma \setminus \{\text{'0', '1', '2'}\}$ corresponds to \lstinline|[^012]*|.

		
\subsection{Mohri-Nederhof Approximation}\label{sec:mohriNederhofApprox}


Mohri and Nederhof \cite{mohri_nederhof} describe an algorithm to approximate a given \ac{cfg} with an \ac{srg}.

Recall from the definition of \acp{srg} in Section \ref{sec:background:srg}, that we can partition the nonterminals of a grammar into equivalence classes based on whether they are reachable from each other. Also recall that a grammar is strongly regular, if for each such equivalence class, all recursive productions of nonterminals contained in it are either left-linear or right-linear.

For determining if a production rule of a given equivalence class is right- or left-linear, all nonterminals that are not part of the class can be considered as terminals. For example a production $A \rightarrow CX$ where $A$ and $C$ are nonterminals in the same equivalence class and $X$ is a nonterminal in another class is left linear because $X$ can be viewed as a terminal.

To transform a \ac{cfg} into an \ac{srg}, we only need to transform the sets of mutually recursive nonterminals, where not all productions are either left-linear or right-linear.

\subsubsection{Transformation}

Mohri and Nederhof describe a more general approach for transforming the required equivalence classes, that accounts for productions with an arbitrary number of nonterminals on the right hand side \cite{mohri_nederhof}. Since all productions we use have either one or two nonterminals or exactly one terminal on the right hand side, we can reduce this more general approach to the following algorithm described by Christensen et al. \cite{brics}.

The approach to transform a given equivalence class $M$ consists of the following two steps:

First, for each nonterminal $A$ in $M$ add a new nonterminal $A'$. Intuitively, $A'$ represents a sequence of characters immediately following the sequence recognized by $A$. If $A$ is the start terminal of our grammar and therefore corresponds to the analyzed hotspot, add a production $A' \rightarrow \epsilon$.

Second, replace all productions of $A$ according to the replacement rules shown in Figure \ref{fig:approx:prodreplacement}. Here $B$ and $C$ are nonterminals in $M$, $X$ and $Y$ are any nonterminals in a different equivalence class and $R$ is a newly created nonterminal.

\begin{figure}[!h]
	\begin{alignat*}{4}
		& A \rightarrow X 	 && \rightsquigarrow \quad A \rightarrow X\ A'\ \  & &\\
		& A \rightarrow B 	 && \rightsquigarrow \quad A \rightarrow B,\ \ 	   & &B' \rightarrow A'\ &&\\
		& A \rightarrow X\ Y && \rightsquigarrow \quad A \rightarrow R\ A',\ \ & &R  \rightarrow X\ Y\ &&\\
		& A \rightarrow X\ B && \rightsquigarrow \quad A \rightarrow X\ B,\ \  & &B' \rightarrow A'\ &&\\
		& A \rightarrow B\ X && \rightsquigarrow \quad A \rightarrow B,\ \ 	   & &B' \rightarrow X\ A'\ &&\\
		& A \rightarrow B\ C && \rightsquigarrow \quad A \rightarrow B,\ \     & &B' \rightarrow C,\ &&C' \rightarrow A'\\
		& A \rightarrow \texttt{terminal} && \rightsquigarrow \quad A \rightarrow R\ A',\ \ & &R \rightarrow \texttt{terminal}\ &&\\
		& A \rightarrow op(X) && \rightsquigarrow  \quad A \rightarrow R\ A',\ \ & &R \rightarrow op(X)\ &&\\
	\end{alignat*}
\caption{Production replacement rules for regular approximation}
\label{fig:approx:prodreplacement}
\end{figure}
Since all newly created productions are right-linear, after applying this transformation to all components where it is required, all components in the grammar either contain only left- or only right-linear productions. Therefore the resulting grammar is strongly regular.

For example, consider a nonterminal $A$ that is part of the currently transformed component and has two productions $A \rightarrow X B$ and $A \rightarrow B X$, again with $B$ being in the same component as $A$ and $X$ in a different component. Here, the first production is right-linear and the second production is left-linear, so the grammar is not strongly regular.
Now these productions are replaced according to the fourth and fifth rules in Figure \ref{fig:approx:prodreplacement}, which gives us the following productions for $A$: $A \rightarrow XB,\ B' \rightarrow A',\ A \rightarrow B$ and $B' \rightarrow X A'$.
The new productions of $A$ are all right-linear and therefore the grammar can be strongly regular.

\subsubsection{Implementation}

We can again view a grammar as a directed graph as described in Section \ref{sec:charsetApprox}.

The notion of mutual \enquote{reachability}, by which the relation $\mathcal{R}$ defined in Section \ref{sec:background:srg} groups the nonterminals, corresponds to \acp{scc} in this graph view of the grammar.

If two nonterminals $A$ and $B$ are mutually reachable in the graph and therefore part of the same \ac{scc}, there is a sequence of productions to produce $B$ from $A$ and vice versa, which, by definition of $\mathcal{R}$, means they are in the same equivalence class of $\mathcal{R}$.

Thus, to approximate a grammar, we view it as a directed graph and find its \acp{scc}, determine the components, where not all productions are of the same linearity and apply the transformation described above to those components.

\begin{comment}

	
	Since a \ac{srg} generates a regular language and both steps in the transformation to a regular expression create an equivalent output from their input, the resulting regular expression accepts the language generated by the original \ac{srg}.
\end{comment}


\section{Strongly Regular Grammar to Automaton}\label{sec:nederhofAlgorithm}

In the previous section, we obtained a \acl{srg}. This grammar is now converted into a regular expression by first transforming into an automaton like described in this Section. Section \ref{sec:nfa2regex} then describes how this automaton is converted to a regular expression.

\subsection{Algorithm}

Nederhof describes an algorithm to transform an \ac{srg} into an equivalent $\epsilon$-\ac{nfa} \cite{nederhof}. The generated automaton always accepts the same language as the given grammar.

The full algorithm can be seen in Algorithm \ref{nederhofAlg}. It creates an \ac{nfa} $(K,\Sigma, \Delta, s, F)$ with a set of states $K$, an alphabet $\Sigma$, a set of transitions $\Delta$, an initial state $s$ and a set of accepting states $F$ from a given \ac{srg} $(\Sigma, N, P, S)$ with an alphabet $\Sigma$, a set of nonterminals $N$,  a set of productions $P$ and a start nonterminal $S$.

\begin{algorithm}[!h]
	\caption{Nederhof Algorithm: \ac{srg} $(\Sigma, N, P, S)$ $\rightarrow$ \ac{nfa} $(K,\Sigma, \Delta, s, F)$}
	\label{nederhofAlg}
	\begin{algorithmic}[1]
		\State $\mathbf{let}$ $\Delta = \emptyset; s = \texttt{create\_state()}; f = \texttt{create\_state()}; F = \{f\}; K = \{s, f\}$
		\State \Call{make\_fa}{$s, S, f$} \label{alg:initialCall}
		
		\Procedure{make\_fa}{$q_0, \alpha, q_1$}
		\If{$\alpha = \epsilon$} \label{alg:eps}
		\State  $\mathbf{let}$ $\Delta = \Delta \cup {(q_0, \epsilon, q_1)}$ \Comment{add $\epsilon$ transition from state $q_0$ to state $q_1$}
		\ElsIf{$\alpha = a,\mathbf{\ some\ }a \in \Sigma^*$}
		\State  $\mathbf{let}$ $\Delta = \Delta \cup {(q_0, \alpha, q_1)}$ \label{alg:singleTerm}
		\ElsIf{$\alpha = X\beta,\mathbf{\ some\ } X \in V, \beta \in V^* \mathbf{\ such\ that \ }|\beta|>0$}
		\State  $\mathbf{let}$ $q = \texttt{create\_state()};$
		\State \hphantom{$\mathbf{let}$} $K = K \cup \{q\}$ \Comment{create some new state $q$ and add it to the automaton}
		\State \Call{make\_fa}{$q_0,X,q$}
		\State \Call{make\_fa}{$q, X,q_1$}
		\Else
		\State  $\mathbf{let}$ $A = \alpha$ \Comment{$\alpha$ must be a single nonterminal}
		\If{$A \in N_i \mathbf{\ some\ } i$}
		\For{$B \in N_i$}
		\State $\mathbf{let}$ $q_B = \texttt{create\_state()};\ K = K \cup \{q_B\}$
		\EndFor
		\If{$recursive(N_i) = left$}
		\For{$(C \rightarrow X_1 ... X_m) \in P$ $\mathbf{such\ that}$ $C \in N_i \land X_1, ..., X_m \notin N_i$}
		\State \Call{make\_fa}{$q_0,X_1 ... X_m,q_C$} \label{alg:left:firstrec}
		\EndFor
		\For{$(C \rightarrow DX_1 ... X_m) \in P$ $\mathbf{such\ that}$ $C, D \in N_i \land X_1, ..., X_m \notin N_i$} \label{alg:recnt:left}
		\State \Call{make\_fa}{$q_D,X_1 ... X_m,q_C$}
		\EndFor
		\State  $\mathbf{let}$ $\Delta = \Delta \cup {(q_A, \epsilon, q_1)}$ \label{alg:left:eps}
		\Else
		\For{$(C \rightarrow X_1 ... X_m) \in P$  $\mathbf{such\ that}$ $C \in N_i \land X_1, ..., X_m \notin N_i$}
		\State \Call{make\_fa}{$q_C, X_1 ... X_m,q_1$} \label{alg:right:firstrec}
		\EndFor
		\For{$(C \rightarrow X_1 ... X_mD) \in P$  $\mathbf{such\ that}$ $C, D \in N_i \land X_1, ..., X_m \notin N_i$} \label{alg:recnt:right}
		\State \Call{make\_fa}{$q_C,X_1 ... X_m,q_D$}
		\EndFor
		\State  $\mathbf{let}$ $\Delta = \Delta \cup {(q_0, \epsilon, q_A)}$ \label{alg:right:eps}
		\EndIf
		\Else
		\For{$(A \rightarrow \beta)$} \Comment{A is not recursive} \label{alg:nonrecloop}
		\State \Call{make\_fa}{$q_0,\beta,q_1$}  \label{alg:nonrec}
		\EndFor
		\EndIf
		\EndIf
		\EndProcedure
		
	\end{algorithmic}
\end{algorithm}

Note that, for the general algorithm an operation production of form $A \rightarrow op(X)$ is treated like a unary production of form $A \rightarrow X$. The operation productions are always handled by one of the loops in lines \ref{alg:left:firstrec}, \ref{alg:right:firstrec} or \ref{alg:nonrecloop}, because for any operation production initially contained in a cycle, the cycle is broken up by the character set approximation described in Section \ref{sec:charsetApprox}.
Therefore, e.g. an operation production $C \rightarrow op(D)$ with $C$ and $D$ in the same \ac{scc} can no longer occur.
We provide a detailed description of resolving the effects of the operations in Section \ref{sec:opProduction}.

The $\textproc{MAKE\_FA}$ procedure takes two states $q_0$ and $q_1$ and a sequence $\alpha$ of symbols - terminals and nonterminals - and creates an automaton equivalent to the grammar defined by $\alpha$ between those two states.

This recursive process is started in line \ref{alg:initialCall} with the start nonterminal $S$, a newly created initial state $s$ as $q_0$ and a newly created accepting state $f$ as $q_1$.

For single terminals and $\epsilon$ the algorithm adds an according edge between the two nodes $q_0$ and $q_1$ in lines \ref{alg:eps} to \ref{alg:singleTerm}.
Here our implementation differs from the original definition because we allow strings and regular expressions as terminals, whereas usually terminals are single characters. We generalize Nederhof's definition by allowing $a \in \Sigma^*$ instead of just $a \in \Sigma$.
This changes the type of the generated \ac{nfa} because it contains edges labeled with multi-character strings instead of just single symbols. This generalization is possible, because we use the resulting automaton as an input to the state elimination algorithm to create a regular expression in Section \ref{sec:nfa2regex}. This algorithm uses a generalized \ac{nfa} definition with regular expressions - and therefore also strings - as edge labels. If we want to use the \ac{nfa} directly, we can convert it to a usual \ac{nfa} by replacing an edge labeled with a string of length $n$ with $n$ edges chained together using new intermediate states.

When $\alpha$ contains multiple symbols, a new state $q$ is created and the automaton for the first symbol in $\alpha$ is inserted between $q_0$ and $q$ and the one for the rest of $\alpha$ between $q$ and $q_1$. Note that for our use case the rest of alpha always contains at most 1 nonterminal since our productions have at most 2 nonterminals on their right hand side.

If $\alpha$ consists of just a single terminal $A$, that is not part of any set of mutually recursive nonterminals, so from $A$ there is no sequence of productions to reach $A$ again, we just continue the recursion with the right hand sides of $A$'s productions. The created automaton does not need edges or states corresponding to those single non-recursive nonterminals. 


\begin{figure}[!h]
	\begin{minipage}[b]{.45\linewidth}
		\begin{align*}
			&A \rightarrow B\\
			&A \rightarrow C\\
			&B \rightarrow b\\
			&C \rightarrow c
		\end{align*}
		\caption{Example grammar with no recursion}
		\label{fig:nonrec:grammar}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{.45\linewidth}
		\begin{tikzpicture}[
			every initial by arrow/.style = {
				thick,-stealth
			}]
			\node (q0) [state, initial,
			initial text = {}] {$q_0$};
			\node (q1) [state, accepting, right = of q0] {$q_1$};
			\path [-stealth, thick]
			(q0) edge [bend right] node[below] {c}   (q1)
			(q0) edge [bend left] node[above] {b}   (q1);
		\end{tikzpicture}
		\caption{Resulting automaton for the grammar in Figure \ref{fig:nonrec:grammar}}
		\label{fig:nonrec:automaton}
	\end{minipage}
\end{figure}


To explain why this is the case, consider the grammar in Figure \ref{fig:nonrec:grammar} creating just the two words \enquote{$b$} and \enquote{$c$}. Here $A$, $B$ and $C$ are non-recursive nonterminals, so in the initial procedure call with arguments $(q_0, A, q_1)$ there are just the two recursive calls $\textproc{MAKE\_FA}(q_0, B, q_1)$ and $\textproc{MAKE\_FA}(q_0, C, q_1)$ in line \ref{alg:nonrec}. For those calls again the non-recursive case is chosen, such that for the next recursions $\alpha$ equals $b$ or $c$ respectively, which leads to the corresponding edges being created in line \ref{alg:singleTerm}. As demonstrated no edges or states are created for any of the 3 nonterminals, only for the two terminals $a$ and $b$ and the resulting automaton in Figure \ref{fig:nonrec:automaton} accepts the correct language.

For the last remaining case, where $\alpha$ consists of a single nonterminal $A$ that is part of some set of mutually recursive nonterminals $N_i$, the algorithm first adds a new state for each nonterminal in $N_i$ to the graph.

Then we differentiate according to the recursion type of $N_i$, which is obtained by the call to $recursive(N_i)$.

Note that sets with neither left nor right recursion can be handled by either case.

Now for all productions where the left hand side is a nonterminal in $N_i$, a recursive call depending on the right hand side of the production is performed. 

\subsubsection{Definition of the case for right recursive components}

Nederhof only defines the case for left recursion in his publication and states that the else part is \enquote{the converse of the then part} \cite{nederhof}. This suggests that besides switching the condition for the second loop from $C \rightarrow DX_1 \ldots X_m$ to  $C \rightarrow X_1 \ldots X_mD$, switching the order of the states passed to the recursive calls suffices for handling the right recursive case.

However, only changing e.g. $\textproc{MAKE\_FA}(q_0, X_1 \ldots X_m, q_C)$ to $\textproc{MAKE\_FA}(q_C, X_1 \ldots X_m, q_0)$ leads to incorrect results. Applying this version of the algorithm to a fully right recursive grammar returns a graph with correct states and correct edges, with the only difference to a correct solution being that the start and the end state are switched. To get correct results, besides switching the argument order, all occurrences of $q_0$ as an argument to recursive calls need to be replaced with $q_1$ and vice-versa $q_1$ with $q_0$. Algorithm \ref{nederhofAlg} shows this corrected version of Nederhof's algorithm.

To explain the differences between the recursive calls in the different cases, consider the grammars in Figures \ref{fig:rec:grammar:left} and \ref{fig:rec:grammar:right} and the corresponding automata in Figures \ref{fig:rec:automaton1} and \ref{fig:rec:automaton2}.

The inverting of the states in the recursive calls leads to the edges between states $q_2$ and $q_3$ of the automata being inverted, which has no influence on the accepted language. 

Switching $q_0$ and $q_1$ in the recursive calls is what leads to the needed difference in the resulting automata. In the case of the left recursive grammar, any production sequence of $n$ applications of $B \rightarrow Ab$ has to end with replacing the $A$ on the left hand side of the resulting word with the terminal $a$ to finalize the production rule application. This means that each word has to start with $a$, which is realized in the automaton by adding an edge labeled with $a$ from $q_0$ to $q_3$ due to the recursive call in line \ref{alg:left:firstrec}. For the right recursive grammar conversely, each word has to end with an $a$ due to the $b$s being generated on the left hand side of the $A$ in $B \rightarrow bA$. Therefore an edge from $q_3$ to the final state $q_1$ is being added by the recursive call in line \ref{alg:right:firstrec}.
Accordingly the corresponding $\epsilon$-edges are added in lines \ref{alg:left:eps} and \ref{alg:right:eps}.


\begin{figure}[h]
	\begin{minipage}[b]{.45\linewidth}
		\begin{align*}
			&A \rightarrow a\\
			&A \rightarrow B\\
			&B \rightarrow Ab
		\end{align*}
		\caption{Example grammar with left recursion}
		\label{fig:rec:grammar:left}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{.45\linewidth}
		\begin{tikzpicture}[
			every initial by arrow/.style = {
			thick,-stealth
			}]
			\node (q0) [state, initial,
			initial text = {}] {$q_0$};
			\node (q3) [state, right = of q0] {$q_3$};
			\node (q1) [state, accepting, right = of q3] {$q_1$};
			\node (q2) [state, above = of q3] {$q_2$};
			\path [-stealth, thick]
			(q0) edge node[above] {$a$}   (q3)
			(q3) edge[bend right] node[right] {$b$}   (q2)
			(q2) edge[bend right] node[left] {$\epsilon$}   (q3)
			(q3) edge node[above] {$\epsilon$}   (q1);
		\end{tikzpicture}
		\caption{Resulting automaton for the grammar in Figure \ref{fig:rec:grammar:left}}
		\label{fig:rec:automaton1}
	\end{minipage}
\hfill
	\begin{minipage}[b]{.45\linewidth}
		\begin{align*}
			&A \rightarrow a\\
			&A \rightarrow B\\
			&B \rightarrow bA
		\end{align*}
		\caption{Example grammar with right recursion}
		\label{fig:rec:grammar:right}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{.45\linewidth}
		\begin{tikzpicture}[
			every initial by arrow/.style = {
				thick,-stealth
			}]
			\node (q0) [state, initial,
			initial text = {}] {$q_0$};
			\node (q3) [state, right = of q0] {$q_3$};
			\node (q1) [state, accepting, right = of q3] {$q_1$};
			\node (q2) [state, above = of q3] {$q_2$};
			\path [-stealth, thick]
			(q0) edge node[above] {$\epsilon$}   (q3)
			(q2) edge[bend right] node[left] {$b$}   (q3)
			(q3) edge[bend right] node[right] {$\epsilon$}   (q2)
			(q3) edge node[above] {$a$}   (q1);
		\end{tikzpicture}
		\caption{Resulting automaton for the grammar in Figure \ref{fig:rec:grammar:right}}
		\label{fig:rec:automaton2}
	\end{minipage}
\end{figure}

\subsection{Operation Productions}\label{sec:opProduction}

In the following we use the two Java operations \lstinline|reverse| and \lstinline|replace| as examples for operations with different complexities. Note however, that we did not implement the complete list of operations on strings the Java standard library contains. To fully support the standard library, one has to define the transformation described in this section for each operation. 

As described above, for the Nederhof algorithm, operation productions of form $C \rightarrow op(X)$ are treated like a normal unary production $C \rightarrow X$.

Each operation defines an automaton transformation that changes a given automaton.
The new automaton accepts the language obtained by applying the operation to each word in the language of the input automaton.
Consider an operation $replace[old, new]$ corresponding to the Java call \lstinline|s.replace(old, new)|, that returns a copy of the \lstinline|String s|, where each occurrence of the \lstinline|char old| is replaced with the \lstinline|char new|.
The automaton transformation for $replace[old, new]$ traverses the automaton and replaces each occurrence of $old$ on any edge with $new$.

To apply the effect of the different operations onto the created automaton, we first need to find the sub-automata affected by each operation.

To obtain these sub-automata, we taint all nodes and edges if they are created in recursion calls originating from an operation production. 
If a recursive call of the Nederhof algorithm $\textproc{MAKE\_FA}(q_0, X, q_C)$ in line \ref{alg:left:firstrec} is caused by an operation production $C \rightarrow op_1(X)$, we pass $op_1$ as a taint to the recursive call. All edges and states created further down this recursion path will be tainted with $op_1$. In the resulting \ac{nfa}, for each operation that is part of the given grammar, there's a set of tainted nodes and edges representing the parts of the automaton affected by this operation. These sets form a sub-automaton of the \ac{nfa}, onto which the transformation of the corresponding operation can then be applied.

Consider the grammar in Figure \ref{fig:operations:grammar} and the corresponding automaton in Figure \ref{fig:operations:automaton1}. Here the production $A \rightarrow E$ leads to the creation of the left path including state $q_2$, while for the operation production $A \rightarrow replace[f,x](F)$ the subsequent algorithm calls create the colored path. All colored edges and states are tainted with the $replace$ operation. The created \ac{nfa} has two similar paths, since $A \rightarrow replace[f,x](F)$ is treated like an $A \rightarrow F$ production analogous to $A \rightarrow E$, just that the resulting edges and adjacent states are tainted.

After completing the \ac{nfa} creation, we can collect all tainted nodes and apply the automaton transformation defined by $replace[f,x]$ to this sub-automaton consisting of the states $q_0$, $q_3$ and $q_1$.

As mentioned above, for the $replace[f,x]$ operation this transformation consists of replacing all occurrences of $f$ on tainted edges with $x$, which gives us the automaton in Figure \ref{fig:operations:automaton2} for the given example.

For regular expressions as edge labels the replace operation is more complex.

For example \Verb@.*@, which is created when we encounter an unknown value like user input, matches all strings, and therefore also strings containing $f$. After applying the $replace[f,x]$ operation to these strings, they can never contain an $f$, so therefore the edge label should match all strings that contain no $f$. This can be implemented using a negative character class, so \Verb@.*@ is transformed to \Verb@[^f]@ by the $replace[f,x]$ operation. Similarly, existing character classes need to be transformed, for example \Verb@[abf]@ to \Verb@[abx]@ and \Verb@[^ab]@ to \Verb@[^abf]@.

\begin{figure}[h]
	\begin{minipage}[b]{.3\linewidth}
		\begin{align*}
			&A \rightarrow E\\
			&A \rightarrow replace[f, x](F)\\
			&E \rightarrow eE\\
			&E \rightarrow e\\
			&F \rightarrow fF\\
			&F \rightarrow f
		\end{align*}
		\caption{Example grammar with operation production}
		\label{fig:operations:grammar}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{.3\linewidth}
		\begin{tikzpicture}[
			every initial by arrow/.style = {
				thick,-stealth
			}]
			\node (q0) [state, initial, initial where=above,
			initial text = {}, draw = red, fill = red!30] {$q_0$};
			\node (q2) [state, below left = of q0] {$q_2$};
			\node (q3) [state, below right = of q0, draw = red, fill = red!30] {$q_3$};
			\node (q1) [state, accepting, below left = of q3, draw = red, fill = red!30] {$q_1$};
			\path [-stealth, thick]
			(q0) edge node[above] {$\epsilon$}   (q2)
			(q0) edge [draw = red] node[above] {$\epsilon$}   (q3)
			(q2) edge [loop right] node[right] {$e$}   (q2)
			(q3) edge [loop left, draw = red] node[left] {$f$}   (q3)
			(q2) edge node[above] {$e$}   (q1)
			(q3) edge [draw = red] node[above] {$f$}   (q1);
		\end{tikzpicture}
		\caption{Resulting automaton for the grammar in Figure \ref{fig:operations:grammar}}
		\label{fig:operations:automaton1}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{.3\linewidth}
		\begin{tikzpicture}[
		every initial by arrow/.style = {
				thick,-stealth
			}]
			\node (q0) [state, initial, initial where=above,
			initial text = {}] {$q_0$};
			\node (q2) [state, below left = of q0] {$q_2$};
			\node (q3) [state, below right = of q0] {$q_3$};
			\node (q1) [state, accepting, below left = of q3] {$q_1$};
			\path [-stealth, thick]
			(q0) edge node[above] {$\epsilon$}   (q2)
			(q0) edge node[above] {$\epsilon$}   (q3)
			(q2) edge [loop right] node[right] {$e$}   (q2)
			(q3) edge [loop left] node[left] {$x$}   (q3)
			(q2) edge node[above] {$e$}   (q1)
			(q3) edge node[above] {$x$}   (q1);
		\end{tikzpicture}
		\caption{Automaton in figure \ref{fig:operations:automaton1} after applying operation transformation}
		\label{fig:operations:automaton2}
	\end{minipage}
\end{figure}

For more complex operation transformations like a $reverse$ operation, the operation transformation also includes adding and removing states and edges of the automaton.

Consider the automaton in Figure \ref{fig:reverse:automaton1}, where the colored parts are tainted with the $reverse$ operation.

To apply the reverse operation we first duplicate the tainted subautomaton and create a new state for each contained state. Here $q_4$ is created for $q_0$, $q_5$ is created for $q_1$ and $q_6$ is created for $q_3$. We also duplicate all tainted edges alongside the states.
Now we reverse the direction of all edges in the duplicated subautomaton.
For example, after duplication there was the edge $(q_6, c, q_5)$ because the original automaton had an edge $(q_3,c,q_1)$. This edge is now reversed to give us the edge $(q_5, c, q_6)$ that is present in the final automaton.

After reversing all edges, the subautomaton is connected back to the rest using new $\epsilon$ edges.
Finally all tainted edges between the original states are removed together with all states that are not connected to the automaton anymore after this step.
In the example automata, after removing the tainted edges, $q_3$ is disconnected from all other states and therefore removed.
The resulting automaton can be seen in Figure \ref{fig:reverse:automaton2}.

\begin{figure}[h]
	\begin{minipage}[b]{.2\linewidth}
		\begin{align*}
			&S \rightarrow A\\
			&S \rightarrow reverse(B)\\
			&A \rightarrow aA\\
			&A \rightarrow c\\
			&B \rightarrow bB\\
			&B \rightarrow c
		\end{align*}
		\caption{Example grammar with operation production}
		\label{fig:reverse:grammar}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{.25\linewidth}
		\begin{tikzpicture}[
			every initial by arrow/.style = {
				thick,-stealth
			}]
			\node (q0) [state, initial, initial where=above,
			initial text = {}, fill = red!30] {$q_0$};
			\node (q2) [state, below left = of q0] {$q_2$};
			\node (q3) [state, below right = of q0, fill = red!30] {$q_3$};
			\node (q1) [state, accepting, below left = of q3, fill = red!30] {$q_1$};
			\path [-stealth, thick]
			(q0) edge node[above] {$\epsilon$}   (q2)
			(q0) edge [draw = red] node[above] {$\epsilon$}   (q3)
			(q2) edge [loop right] node[right] {$a$}   (q2)
			(q3) edge [loop left, draw = red] node[left] {$b$}   (q3)
			(q2) edge node[above] {$c$}   (q1)
			(q3) edge [draw = red] node[above] {$c$}   (q1);
		\end{tikzpicture}
		\caption{Resulting automaton for the grammar in figure \ref{fig:reverse:grammar}}
		\label{fig:reverse:automaton1}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{.45\linewidth}
		\begin{tikzpicture}[
			every initial by arrow/.style = {
				thick,-stealth
			}]
			\node (q0) [state, initial, initial where=above,
			initial text = {}] {$q_0$};
			\node (q2) [state, below left = of q0] {$q_2$};
			\node (q5) [state, right = of q0] {$q_5$};
			\node (q6) [state, below right = of q5] {$q_6$};
			\node (q4) [state, below left = of q6] {$q_4$};
			\node (q1) [state, accepting, below right = of q2] {$q_1$};
			\path [-stealth, thick]
			(q0) edge node[above] {$\epsilon$}   (q2)
			(q2) edge [loop right] node[right] {$a$}   (q2)
			(q2) edge node[above] {$c$}   (q1)
			(q0) edge node[above] {$\epsilon$}   (q5)
			(q5) edge node[above] {$c$}   (q6)
			(q6) edge node[above] {$\epsilon$}   (q4)
			(q6) edge [loop left] node[left] {$b$}   (q6)
			(q4) edge node[below] {$\epsilon$}   (q1);
		\end{tikzpicture}
		\caption{Automaton in figure \ref{fig:reverse:automaton1} after applying operation transformation\\}
		\label{fig:reverse:automaton2}
	\end{minipage}
\end{figure}


\section{Automaton to Regular Expression}\label{sec:nfa2regex}

To get a human-readable format for the information we obtained, we transform the automaton we created to a regular expression. Section \ref{sec:stateElimination} describes this conversion and \ref{sec:delgado} presents an optimization we used to improve the results.

\subsection{State elimination}\label{sec:stateElimination}

To transform the automaton we created from an \ac{srg}, we use the state elimination strategy, also known as the Brzozowski-McCluskey procedure \cite{brzozowksi_mccluskey}.

To apply the procedure, an automaton is first transformed into a \ac{gnfa}. A \ac{gnfa} is an \ac{nfa} where the edges are labeled with regular expressions instead of single symbols.
Also a \ac{gnfa} must only have a single start state and a single accepting state \cite{hanGNFA}.

To achieve this characteristic, one can add a new start state with a single $\epsilon$ transition to the old start state and a new final state with incoming $\epsilon$ edges from all previously accepting states.

However, due to the automaton construction using the Nederhof algorithm described above, the automata we obtain already fulfill this property without any need for further modification.
We also already use regular expressions as edge labels from the start.

We first replace each pair of edges $(q_0, r_1, q1), (q_0, r_2, q1)$ between two states with a single edge $(q_0, r_1|r_2, q_1)$.
After applying this replacement rule exhaustively, there are no two states $q_0$ and $q_1$ with more than one direct edge between them.

To eliminate a state $q$, we \enquote{shortcut} the state by replacing all pairs of transitions $(q',r, q), (q, t, q'')$ with a new transition from $q'$ to $q''$.
The new transition is $(q', rt, q'')$ if $q$ has no loop edge to itself and $(q', rs^*t, q'')$ if it has one with label $s$ \cite{esparza}.

Figures \ref{fig:stateelim:rule2} and \ref{fig:stateelim:rule3} contain examples adapted from Esparza \cite{esparza} that visualize those rules.

After repeatedly applying the two rules and eliminating all other states, the resulting automaton contains only the start and the end state. The single edge between those two states then has the resulting regular expression as a label.

\begin{figure}
	\centering
	\begin{tikzpicture}
		every initial by arrow/.style = {
			thick,-stealth
		}]
		\node (q0) [state] {$q_0$};
		\node (q1) [state, right = of q0] {$q_1$};
		\node (arrow) [right = of q1] {$\Longrightarrow$};
		\node (q0') [state, right = of arrow] {$q_0$};
		\node (q1') [state, right = of q0'] {$q_1$};
		\path [-stealth, thick]
		(q0) edge[bend left] node[above] {$r_1$}   (q1)
		(q0) edge[bend right] node[below] {$r_2$}   (q1)
		(q0') edge node[above] {$r_1 | r_2$}   (q1');
	\end{tikzpicture}
	\caption{Replacement of edge pairs}
	\label{fig:stateelim:rule2}.
\end{figure}

\begin{figure}
	\centering
	\begin{tikzpicture}
		every initial by arrow/.style = {
			thick,-stealth
		}]
		\node at (0,3) (q0) [state] {$q_0$};
		\node at (0,0) (q1) [state] {$q_1$};
		\node at (2,1.5) (q4) [state] {$q_4$};
		\node at (4,3) (q2) [state] {$q_2$};
		\node at (4,0) (q3) [state] {$q_3$};
		
		\node at (5.5, 1.5) (arrow) {$\Longrightarrow$};
		
		\node at (7,3) (q0') [state] {$q_0$};
		\node at (7,0) (q1') [state] {$q_1$};
		\node at (11,3) (q2') [state] {$q_2$};
		\node at (11,0) (q3') [state] {$q_3$};
		
		\path [-stealth, thick]
		(q0) edge node[above, xshift=2] {$r_1$} (q4)
		(q1) edge node[below, xshift=2] {$r_n$} (q4)
		
		(q4) edge[loop above] node {$s$} (q4)
		
		edge node[above] {$t_1$} (q2)
		edge node[below] {$r_m$} (q3)
		
		
		(q0') edge node[above] {$r_1s^*t_1$} (q2')
		(q1') edge node[above left,  xshift=7, yshift=-33] {$r_ns^*t_1$} (q2')
		(q0') edge node[below right, xshift=-29, yshift=35] {$r_1s^*t_m$} (q3')
		(q1') edge node[below] {$r_ns^*t_m$} (q3');
	\end{tikzpicture}
	\caption{Elimination of state $q_4$}
	\label{fig:stateelim:rule3}.
\end{figure}

\subsection{Delgado heuristic}\label{sec:delgado}

The resulting regular expressions often do not have minimal length, but there exists no efficient algorithm that always produces optimal regular expressions.

Minimizing regular expressions is PSPACE-complete \cite{minimizing_nfa}.
If there were an efficient algorithm to obtain the minimal regular expression for a given \ac{nfa}, one could first apply Thompson's algorithm \cite{thompson} for turning a regular expression into an equivalent \ac{nfa} and then this algorithm. This chaining would then be an efficient regular expression minimization algorithm, which is not possible as mentioned above.

However, we can still improve the result we obtain using the state elimination method.
The order in which states are eliminated affects the size of the resulting regular expression. There exist different heuristics for choosing an elimination order to reduce the expression size.

We chose a heuristic described by Delgado and Morais \cite{delgado}.
For each state a weight is calculated using Formula \ref{form:delgado}, where $In_q$ is the set of incoming edges of $q$, $Out_q$ the set of outgoing edges, $W_e$ the size of the label on any edge $e$. $Out_q$ and $In_q$ both do not contain a potential loop on $q$. $W_{loop}$ is the size of the loop around $q$ if it exists and $0$ otherwise. 

\begin{equation}
	\begin{aligned}
	weight(q) = &&\sum_{e \in In_q} (W_{e} \times (|Out_q| - 1))\ + \\&&\sum_{e \in Out_q} (W_{e} \times (|In_q| - 1))\ + \\&&W_{loop} \times (|In_q| \times |Out_q| - 1)
	\end{aligned}
\label{form:delgado}
\end{equation}

The weight represents the length of the expression added to the result by removing this state.
Therefore, in each algorithm run, the state with the smallest weight is chosen for elimination.

Delgado and Morais show that using this heuristic produces significantly shorter expressions compared to a naive state elimination \cite{delgado} with random or undefined ordering. Gruber et al. also show it outperforms almost all other heuristics they considered for their comparison \cite{gruber}.
Improving the algorithm further by implementing a look-ahead additionally to the heuristic also improves the results, but adds more complexity and impairs the algorithm's performance \cite{delgado}.

Another reduction of the regular expression size can often be obtained by first converting the automaton to a \ac{dfa}, for example using the powerset construction. For a given \ac{nfa} with $n$ states, the \ac{dfa} obtained by using the powerset construction to convert the \ac{nfa} can have up to $2^n$ states. However, we observed that for many \acp{nfa} generated using Nederhof's algorithm, the resulting \acp{dfa} are significantly smaller than the input \ac{nfa} or even minimal. This \ac{dfa} can be minimized using common algorithms like Hopcroft's or Brzozowski's algorithm for even better results.

Since it is unclear in which cases conversion to \acp{dfa} leads to better results, we can try both approaches for a given query and return the shorter expression to optimize the returned result.


\section{Hotspot Collection} 
We also implemented a new pass that traverses the \ac{cpg} and collects nodes representing string values which might be of interest for further analysis. 
This hotspot collection provides common starting points for our grammar creation to the user. However, the grammar creation is completely independent of this collection and a grammar can be created for any string node, independent of whether it is part of the collection.
We consider all strings that are passed as an SQL query to the Standard Java SQL API, as these are the locations where potential SQL injections can occur.
